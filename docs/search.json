[
  {
    "objectID": "case_study.html#교통사고",
    "href": "case_study.html#교통사고",
    "title": "\n1  사례 연구\n",
    "section": "\n1.1 교통사고",
    "text": "1.1 교통사고\n포아송은 미래에 발생할 경우의 수를 예측하기 위해서 포아송 분포를 창안했다. 좀더 구체적으로 고정된 시간 범위에 발생할 사건을 예측하기 위해서다.\n한가지 사례로 한국 R 사용자회 페이스북 그룹에 매주 페이스북 게시글을 올리는데 좋아요를 누르는 평균 회원수가 10명이다. 새로운 페이스북 게시글을 올렸는데 좋아요를 누른 회원이 15명이 될 확률은 얼마나 될까?\n이와 같이 다음주(미래) 좋아요를 누른(사건) 회원수가 15명(5, 10, 20, …)이 될 확률을 알고 싶은 것이다.\n\n1.1.1 포아송 분포 수렴\n다음 조건을 만족할 때 이항분포가 포아송 분포로 수렴되어 근사할 수 있다.\n\n시행 횟수 \\(n\\) 이 매우 크다.\n성공 확률 \\(p\\) 가 매우 작다.\n따라서, \\(\\lambda = n \\times p\\) 가 일정하다.\n\n이항분포 \\(Bin(n, p)\\)는 포아송 분포 \\(Poi(\\lambda)\\)에 근사한다.\n전체 제품 중에서 고장확률이 매우 작은 전자제품을 사례로 들어보자. 예를 들어, 어떤 공장에서 10,000개의 제품을 제조했을 때, 각 제품이 고장날 확률이 0.0001이라고 가정하면 이항분포로 전체 제품 중 1개 고장확률을 계산할 수 있지만, 제품 수가 매우 크고 고장 확률이 매우 작기 때문에 \\(\\lambda = np = 10,000 \\times 0.0001 = 1\\)를 갖는 포아송 분포를 사용하여 근사할 수 있다.\n\n1.1.2 월간 교통사고\n한 도시의 주요 교차로에서, 지난 1년 동안의 데이터를 기반으로 하루 평균 3건의 교통 사고가 발생했다고 가정하자. 이 정보를 바탕으로 특정 날에 교통 사고가 발생할 횟수의 확률 분포를 예측해보자.\n포아송 분포의 평균은 \\(\\lambda\\)이며, 이 경우에는 하루 평균 교통 사고 횟수인 3으로 설정할 수 있다.\n이제 포아송 분포의 확률 질량 함수를 사용하여, 특정 날에 교통 사고가 k번 발생할 확률을 계산할 수 있다.\n\\[\nP(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\n여기서, \\(e\\)는 자연상수입니다.\n예를 들어, 특정 날에 교통 사고가 정확히 2번 발생할 확률을 계산하려면:\n\\[P(X=2) = \\frac{3^2 e^{-3}}{2!} = 0.224\\]\n한걸음 더 들어가 실세 교통사고분석시스템(TAAS) 웹사이트에서 2022년 월별 교통사고 데이터를 얻을 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\naccident_raw &lt;- read_excel(\"data/2022년_월별_교통사고.xlsx\", sheet = \"2022년도 월별 교통사고\", skip = 2)\n\naccident_tbl &lt;- accident_raw |&gt; \n  janitor::clean_names(ascii = FALSE) |&gt; \n  select(월, 사고건수 = 사고건수_건) \n\naccident_tbl\n\n# A tibble: 12 × 2\n   월    사고건수\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 01월     15894\n 2 02월     12362\n 3 03월     13620\n 4 04월     16472\n 5 05월     18065\n 6 06월     16481\n 7 07월     17115\n 8 08월     16496\n 9 09월     17216\n10 10월     18508\n11 11월     17578\n12 12월     17029\n\n\n월별 평균 사고건수를 mean() 함수를 사용해서 계산할 수 있다. 교통량은 상당히 크고, 교통사고 확률은 매우 낮기 때문에 포아송 분포로 근사를 하는 것이 가능하다. 교통사고 건수가 많기 때문에 단위를 천대 기준으로 조정하여 포아송 분포 모수 \\(\\lambda\\)를 계산한다.\n\naccident_mean &lt;- mean(accident_tbl$사고건수) / 1000\n\naccident_mean\n\n[1] 16.403\n\n\n월별로 2만대 이상 교통사고가 발생될 경우 교통사고 환자수가 급증하여 병원에 큰 부하가 걸려 사회적 문제가 된다는 가정하에 월별로 2만대 이상 교통사고가 발생할 확률을 계산해보자.\n\\(P(X \\geq 20)\\) 확률값은 전체 경우의 수에서 0 ~ 1.9 만대 사고건수가 발생할 확률을 빼주면 계산이 가능하고 다음과 같이 수식으로 표현할 수 있다.\n\\[\nP(X \\geq 20) = 1 - (P(X=0) + P(X=1) + \\ldots + P(X=19))\n\\]\n이를 R 코드로 작성하면 다음과 같이 함수형 프로그래밍 purrr 패키지 map_dbl() 함수와 포아송 함수에 \\(\\lambda = 16.403\\)를 넣어 계산이 가능하거나 내장 함수 ppois()로 직접 동일한 계산작업을 수행할 수 있다.\n\nlibrary(tidyverse)\n\naccident_prob &lt;- 1 - sum(map_dbl(0:19, ~ (accident_mean^.x * exp(-accident_mean)) / factorial(.x)))\n# 1 - ppois(19, lambda = 16.403)\n\naccident_prob\n\n[1] 0.2169535"
  },
  {
    "objectID": "case_study.html#프로이센-기병-사망자",
    "href": "case_study.html#프로이센-기병-사망자",
    "title": "\n1  사례 연구\n",
    "section": "\n1.2 프로이센 기병 사망자",
    "text": "1.2 프로이센 기병 사망자\n프로이센 기병대에서 말 발길질로 사망한 병사의 수를 1875년부터 1894년까지, 14개의 기병 군단을 대상으로 수집한 데이터(Prussian Horse-Kick Data)가 포아송 분포에 잘 적합되는 것으로 유명하다.\n\n\n\n\n원본 데이터를 디지털로 복원한 후에 고정된 기간 말 발차기 사망자수를 빈도통계를 통해 표로 정리할 수 있다. 총 관측 횟수는 \\(14 \\times 20 = 280\\) (즉, 1875년부터 1894년까지 20년간 프로이센 군단 14개를 관측), 총 사망자 병사수가 196명으로부터 평균 사망병사수를 \\(\\lambda = \\frac{196}{280} = 0.7\\) 으로 계산할 수 있다. 다음으로 포아송분포에 적합시켜서 분포로부터 말 발차기 사망자수 빈도수를 계산한다.\n\nlibrary(rvest)\nlibrary(gt)\nlibrary(gtExtras)\n\nkick_raw &lt;- read_html(x = 'https://www.randomservices.org/random/data/HorseKicks.html') |&gt; \n  html_node(\"table\") |&gt; \n  html_table()\n\n# kick_raw |&gt; \n#   write_csv(\"data/horse_kick.csv\")\n\nkick_tbl &lt;- kick_raw |&gt; \n  pivot_longer(-Year, names_to = \"군단\", values_to = \"병사수\") |&gt; \n  count(사망횟수 = 병사수, name = \"빈도수\") |&gt; \n  mutate(사망자수 = 사망횟수 * 빈도수)  |&gt; \n  mutate(포아송적합 = map_dbl(사망횟수, dpois, lambda = 196/280) * 280) |&gt; \n  mutate(포아송적합 = round(포아송적합, digits = 0)) |&gt; \n  janitor::adorn_totals(c(\"row\"), name = \"합계\")\n\nkick_tbl |&gt; \n  gt() |&gt; \n  gt_theme_538() |&gt; \n  cols_align(\"center\") |&gt; \n  gt::tab_spanner(label = \"데이터\", \n                  columns = c(사망횟수, 빈도수))\n\n\n\n\n\n\n\n        데이터\n      \n      사망자수\n      포아송적합\n    \n\n사망횟수\n      빈도수\n    \n\n\n\n0\n144\n0\n139\n\n\n1\n91\n91\n97\n\n\n2\n32\n64\n34\n\n\n3\n11\n33\n8\n\n\n4\n2\n8\n1\n\n\n합계\n280\n196\n279\n\n\n\n\n\n\n시각적으로 실제 관측한 빈도수와 포아송 분포로부터 추정한 값을 함께 겹칠 경우 일부 차이가 있긴 하지만 대체로 포아송 분포에 잘 적합됨을 확인할 수 있다.\n\nkick_tbl |&gt; \n  filter(사망횟수 != \"합계\") |&gt; \n  ggplot() +\n    geom_segment(aes(x = 사망횟수, xend = 사망횟수, y = 0, yend=빈도수),\n                 linewidth= 2) +\n    geom_point(aes(사망횟수, 포아송적합), size=3, color=\"red\") +\n    labs(x = \"말 발차기로 사망한 병사 수\",\n         y = \"빈도수\",\n         title = \"말 발길질로 인한 프로이센 병사 사망\",\n         subtitle = \"실제 관측 데이터와 포아송분포 적합 기대값\")"
  },
  {
    "objectID": "clt.html",
    "href": "clt.html",
    "title": "\n3  중심극한정리 예시\n",
    "section": "",
    "text": "4 중심극한정리\n상자 안에 1에서 999까지 숫자가 표시된 999개의 상태가 균질한 공(\\(X\\))을 넣고 이를 특정한 모집단(population)이라고 가정하자. 이 모집단의 평균 \\(\\mu\\)은 500이다. 모집단의 분산 \\(\\mathrm{var}(X)\\)는 80,475이다.\n이 중 30개의 공을 50회에 걸쳐 반복 추출한다. 이 경우 표본평균의 분포는 \\(E(\\bar{X}_i)=\\mu(X)\\)이고 분산이 \\(\\mathrm{var}(\\bar{X_i})=\\frac{\\sigma^2}{n}\\)인 정규분포에 근사한다. 즉, \\(X \\sim \\mathcal{N}(500,~283.7^2)\\)이다."
  },
  {
    "objectID": "clt.html#이를-실험을-통해-살펴보자.",
    "href": "clt.html#이를-실험을-통해-살펴보자.",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n5.1 이를 실험을 통해 살펴보자.",
    "text": "5.1 이를 실험을 통해 살펴보자.\n999개의 공이 든 상자에서 30개의 공을 50회에 걸쳐 무작위 반복추출하고(30개의 공을 뽑은 뒤, 다시 그 공을 상자 안에 집어 놓고 상자를 처음과 같은 상태가 되도록 뒤흔들어서 다시 30개의 공을 뽑는 것을 50회 반복한다), 그 각각의 평균을 기록하면 다음과 같다.\n\n\n50회 반복추출의 평균값: 361.8, 587.8, 496.9, 535.6, 523.3, 634.7, 459.8, 488.7, 447.6, 560.4, 513.1, 440.7, 449.2, 431.0, 475.1, 458.3, 552.7, 465.0, 489.6, 484.3, 485.1, 527.1, 552.3, 549.4, 468.7, 500.8, 552.3, 464.9, 514.8, 445.4, 474.9, 488.3, 545.7, 591.5, 416.3, 521.8, 516.3, 498.5, 490.0, 615.1, 582.4, 458.4, 452.2, 513.4, 499.9, 467.7, 438.3, 521.1, 550.5, 622.5\n\n\n표본평균값의 분포를 히스토그램으로 표현하면 그림 5.1와 같다.\n\n\n\n\n그림 5.1: 표본평균의 히스토그램"
  },
  {
    "objectID": "clt.html#그림에서-보듯",
    "href": "clt.html#그림에서-보듯",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n5.2 그림에서 보듯",
    "text": "5.2 그림에서 보듯\n\\(\\bar{X_i}\\)는 500을 중심으로 좌우대칭적으로 분포하고 있다. 이들의 평균은 503.6으로 모평균 500과 근사하다. 이들 중 이론적으로 산출한 평균으로부터 약 2 표준편차만큼 떨어진 구간(\\(\\mu \\pm 2\\frac{\\sigma}{\\sqrt{n}}\\)) 안에 속하는 값, 즉 \\([396.4, 603.6]\\)의 범위 안에 있는 값의 개수를 세면 모두 46개이다.\n표본평균 분포의 약 95%를 포괄하고 있음을 알 수 있다."
  },
  {
    "objectID": "clt.html#r-markdown",
    "href": "clt.html#r-markdown",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n5.3 R Markdown",
    "text": "5.3 R Markdown\nRender 버튼을 누르면 문서가 생성된다. 여기에는 내용과 함께 내장된 R 코드 청크가 실행된다. R 코드는 다음과 같이 포함할 수 있다.\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "clt.html#플로트-포함하기",
    "href": "clt.html#플로트-포함하기",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n5.4 플로트 포함하기",
    "text": "5.4 플로트 포함하기\n플로트도 포함할 수 있으니, 다음과 같다.\n\n\n\n\n그림 5.2: Pressure\n\n\n\necho = FALSE 파라미터를 주었기 때문에 이 플로트를 생성하는 R 코드가 인쇄되지 않는다. 그림 5.2을 보자."
  },
  {
    "objectID": "clt.html#재미있는-배열-문제",
    "href": "clt.html#재미있는-배열-문제",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n5.5 재미있는 배열 문제",
    "text": "5.5 재미있는 배열 문제\n\nN, C = 13,3\na=[ f\"{i+1}\" for i in range(N) ]\nfor i in range(1, C):\n    if len(a)%C == i: a.insert((len(a)//C+1)*(i+1)-1, \" \")\nprint(\"\\\\begin{tabular}{%s}\"%(\"l\"*C))\n\n\\begin{tabular}{lll}\n\nfor i in range(len(a)//C): print(\" & \".join(a[i::len(a)//C]), \" \\\\\\\\\")\n\n1 & 6 & 10  \\\\\n2 & 7 & 11  \\\\\n3 & 8 & 12  \\\\\n4 & 9 & 13  \\\\\n5 &   &    \\\\\n\nprint(\"\\\\end{tabular}\")\n\n\\end{tabular}\n\n\n이 문제는 흥미롭다. KTUG 게시판에 올라온 문제에 대하여 aud라는 분이 단 답변이다. 한편 로도 같은 일을 할 수 있음이 답글 중에 제시되어 있다."
  },
  {
    "objectID": "clt.html#개관",
    "href": "clt.html#개관",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n6.1 개관",
    "text": "6.1 개관\nQuarto의 특징 중의 하나는  문서의 소스를 그대로 집어넣어도 된다는 것이다. 이 장의 텍스트는 다른 곳에서 작성한  소스를 복사한 것이다."
  },
  {
    "objectID": "clt.html#표-그리기",
    "href": "clt.html#표-그리기",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n6.2 표 그리기",
    "text": "6.2 표 그리기\n다른 곳에서 책을 하나 조판하던 때에, tabular의 괘선에 색을 입혀달라는 요구가 있었다. 2020년경이었는데, 당시로서 이것을 구현하는 것은 거의 불가능해 보였으나 어찌어찌 tabular 자체 코드를 해킹해서 어렵사리 성공했더랬다. 그리고 잠시 지났더니 가 나왔다. 조금 더 일찍 나왔다면 그 고생을 하지 않았을 것 아닌가!\n이 패키지를 사용하면 그동안 골칫거리였던 tabular 관련 문제가 대부분 해결된다. 사용법이 조금 복잡해보일지 모르지만 익숙해지면 편하게 쓸 수 있다."
  },
  {
    "objectID": "clt.html#footnotes-in-boxed-environment",
    "href": "clt.html#footnotes-in-boxed-environment",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n6.3 footnotes in boxed environment",
    "text": "6.3 footnotes in boxed environment\n의 apparatus 중에 minipage footnote라는 것이 있다. 예를 들면 다음과 같은 것이다.\n이것은 매우 유용한 장치이기는 하나, 단행본을 출간하는 입장에서 가끔 모든 각주를 페이지 하단에 넣으라는 요구를 받을 때가 있다. 가장 간단한 해결책은 명령을 와 로 분해하는 것이다."
  },
  {
    "objectID": "clt.html#문헌목록",
    "href": "clt.html#문헌목록",
    "title": "\n3  중심극한정리 예시\n",
    "section": "\n6.4 문헌목록",
    "text": "6.4 문헌목록\n참고 문헌 인용과 목록 생성 실험을 합니다. 한국어 문헌과 구미어 문헌은 그 목록형성과 인용 방법이 다릅니다. 한국어 문헌의 예를 들면, [@kimuycwung_hankwukphan_2003]같고, 영어 문헌은 예를 들면, [@Allport:1992:OND]같습니다."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hendron, K. (2016, April 20). Germany’s u-boats & data\nvisualization: Can data visualization offer answers to our questions\nabout history? https://medium.com/@kadenhendron/germany-s-u-boats-data-visualization-6e018c6c174\n\n\nMarwick, B., Boettiger, C., & Mullen, L. (2018). Packaging data\nanalytical work reproducibly using r (and friends). The American\nStatistician, 72(1), 80–88.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680. http://www.jstor.org/stable/1671815\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for\ndata science. \" O’Reilly Media, Inc.\".\n\n\nWiener, N. (1921). A new theory of measurement: A study in the logic of\nmathematics. Proceedings of the London Mathematical Society,\n2(1), 181–205.\n\n\n이경화. (2020). 고등학교 실용통계. 통계청 통계교육원."
  },
  {
    "objectID": "case_study.html#v2-rocket",
    "href": "case_study.html#v2-rocket",
    "title": "\n1  사례 연구\n",
    "section": "\n1.3 런던 투하 V2 로켓",
    "text": "1.3 런던 투하 V2 로켓\n2차 세계대전 중 독일은 신형 무기 V1, V2 로켓을 개발하여 전쟁 막판에 영국 런던을 폭격하여 반전을 노렸다. 독일 신형폭탄의 공격을 받은 영국에서는 독일에서 발사한 신형 폭탄이 정밀 타격한 것인지 아니면 무작위로 대충 발사를 한 것인지 데이터를 통해 검정을 하고자 한다. 1 2\n\n1.3.1 데이터\n가장 먼저 데이터를 준비한다. 데이터는 R.D. Clarke, “An Applicatin of the Poisson Distribution”을 참조한다.\n\n# 1. 기본 데이터 --------- \n\nbombs &lt;- c(\"0 개\",\"1 개\", \"2 개\",\"3 개\",\"4 개\",\"5 개 이상\")\nhit &lt;- c(229, 211, 93, 35, 7, 1)\nexpected &lt;- c(226.74, 211.39, 98.54, 30.62, 7.14, 1.57)\n\nbomb_df &lt;- tibble(bombs, hit, expected)\nbomb_df |&gt; \n  gt() |&gt; \n  gt_theme_538()\n\n\n\n\n\nbombs\n      hit\n      expected\n    \n\n\n0 개\n229\n226.74\n\n\n1 개\n211\n211.39\n\n\n2 개\n93\n98.54\n\n\n3 개\n35\n30.62\n\n\n4 개\n7\n7.14\n\n\n5 개 이상\n1\n1.57\n\n\n\n\n\n\n\n1.3.2 포아송 분포\n런던에 떨어진 폭탄이 포아송 분포, 즉 무작위로 떨어진 것이라고 가정하고 시각화를 한다. 포아송 분포는 모수가 \\(\\lambda\\) 하나만 추정하면 되기 때문에 데이터에서 모수를 추정한다.\n\\[P(\\text{ 해당 구간에서 발생한 k개 사건(k events in interval)}) = e^{-\\lambda}\\frac{\\lambda^k}{k!}\\]\n\n# 2. 포아송 분포 --------- \n\nhit &lt;- 537\narea &lt;- 576\n\n(lambda &lt;- hit/area)\n\n[1] 0.9322917\n\nggplot(bomb_df, aes(x=bombs,xend=bombs, y=0, yend=hit)) +\n  geom_segment(size=1.5) +\n  geom_point(aes(bombs, expected), size=2, color=\"red\") +\n  labs(x=\"런던 지역에 투하된 폭탄 수\", y=\"런던 지역 숫자\", title=\"영국 런던에 떨어진 V2 로켓 폭탄\",\n       subtitle=\"실제 투하 폭탄수와 포아송 분포로 추정한 폭탄수\")\n\n\n\n\n\n\n\n모수(\\(\\lambda\\))는 0.9322917로 추정된다. 이를 실제 데이터와 포아송 분포에서 나온 데이터와 겹쳐 시각화한다.\n예를 들어, 폭탄이 투하되지 않을 확률은 다음과 같다.\n\\[P(x=0) = e^{-0.9322917}\\frac{0.9322917^0}{0!} = 0.3936506\\]\n이를 R 코드로 표현하면 다음과 같다.\n\nlambda^0 *exp(-lambda) / factorial(0)\n\n[1] 0.3936506\n\n\n\n1.3.3 가설 검정\n시각적으로 살펴봤지만, 통계적 가설검정을 통해 다시 한번 런던에 투척된 폭탄이 포아송 분포를 따르는 것인지 검정해본다.\n\n귀무가설(\\(H_0\\)): 런던에 투하된 폭탄은 무작위로 떨어진 것이다. 즉, 폭탄이 떨어진 분포는 포아송 분포다.\n대립가설(\\(H_A\\)): 폭탄이 떨어진 것은 의도를 갖고 특정지역에 투하된 것이다.\n\n유의수준을 설정하고 검정통계량 \\(\\chi^2\\)을 정의해서 계산하면 귀무가설을 채택하게 된다.\n\n# 3. 통계적 검정 --------- \n\nchisq.test(bomb_df$hit, p=bomb_df$expected, rescale.p=TRUE, simulate.p.value=TRUE)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 2000 replicates)\n\ndata:  bomb_df$hit\nX-squared = 1.1709, df = NA, p-value = 0.9505\n\n# 4. 최종 데이터 ---------\n\nbomb_df$r_expected &lt;- 573 * c( dpois(0:4, lambda), 1 - sum(dpois(0:4, lambda)))\n\nbomb_df |&gt; \n  gt() |&gt; \n    gt_theme_538()\n\n\n\n\n\nbombs\n      hit\n      expected\n      r_expected\n    \n\n\n0 개\n229\n226.74\n225.561771\n\n\n1 개\n211\n211.39\n210.289359\n\n\n2 개\n93\n98.54\n98.025509\n\n\n3 개\n35\n30.62\n30.462788\n\n\n4 개\n7\n7.14\n7.100051\n\n\n5 개 이상\n1\n1.57\n1.560522\n\n\n\n\n\n\n\n1.3.4 지리정보를 통한 이해\n공간정보를 활용한 사례로 이를 공간정보에 시각화하면 다음과 같다. 물론 정확한 데이터가 없어 런던 남부에 떨어진 폭탄이 포아송 분포를 따른다고 가정하고 576개 구획으로 나눈 것에 임의로 폭탄이 떨어진 것을 시각화하면 다음과 같다.\n\n# 5. 지리정보 ---------\nlibrary(spatstat)\npar(mar = rep(0, 4))\n\n# 24*24 = 576\nsouth_london &lt;- rpoispp(lambda, win = owin(c(0, 24), c(0, 24)))\nplot(south_london, main=\"\", cex=0.5)\nabline(h = 0:24, v = 0:24, col = \"lightgray\", lty = 3)\n\n\n\n\n\n\n\n포아송 분포를 가정하고 통계적 검정도 물론 가능하다. spatstat 팩키지의 함수를 활용하여 통계적 검정을 해도 동일한 결론에 도달하게 된다.\n\nbomb_test &lt;- quadrat.test(south_london, nx = 24, ny = 24, method=\"Chisq\")\nbomb_test\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  south_london\nX2 = 591.53, df = 575, p-value = 0.6154\nalternative hypothesis: two.sided\n\nQuadrats: 24 by 24 grid of tiles\nTessellation is marked"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPT 통계",
    "section": "",
    "text": "서문"
  },
  {
    "objectID": "cs_prussia.html#교통사고",
    "href": "cs_prussia.html#교통사고",
    "title": "\n1  프로이센 기병\n",
    "section": "\n1.1 교통사고",
    "text": "1.1 교통사고\n포아송은 미래에 발생할 경우의 수를 예측하기 위해서 포아송 분포를 창안했다. 좀더 구체적으로 고정된 시간 범위에 발생할 사건을 예측하기 위해서다.\n한가지 사례로 한국 R 사용자회 페이스북 그룹에 매주 페이스북 게시글을 올리는데 좋아요를 누르는 평균 회원수가 10명이다. 새로운 페이스북 게시글을 올렸는데 좋아요를 누른 회원이 15명이 될 확률은 얼마나 될까?\n이와 같이 다음주(미래) 좋아요를 누른(사건) 회원수가 15명(5, 10, 20, …)이 될 확률을 알고 싶은 것이다.\n\n1.1.1 포아송 분포 수렴\n다음 조건을 만족할 때 이항분포가 포아송 분포로 수렴되어 근사할 수 있다.\n\n시행 횟수 \\(n\\) 이 매우 크다.\n성공 확률 \\(p\\) 가 매우 작다.\n따라서, \\(\\lambda = n \\times p\\) 가 일정하다.\n\n이항분포 \\(Bin(n, p)\\)는 포아송 분포 \\(Poi(\\lambda)\\)에 근사한다.\n전체 제품 중에서 고장확률이 매우 작은 전자제품을 사례로 들어보자. 예를 들어, 어떤 공장에서 10,000개의 제품을 제조했을 때, 각 제품이 고장날 확률이 0.0001이라고 가정하면 이항분포로 전체 제품 중 1개 고장확률을 계산할 수 있지만, 제품 수가 매우 크고 고장 확률이 매우 작기 때문에 \\(\\lambda = np = 10,000 \\times 0.0001 = 1\\)를 갖는 포아송 분포를 사용하여 근사할 수 있다.\n\n1.1.2 월간 교통사고\n한 도시의 주요 교차로에서, 지난 1년 동안의 데이터를 기반으로 하루 평균 3건의 교통 사고가 발생했다고 가정하자. 이 정보를 바탕으로 특정 날에 교통 사고가 발생할 횟수의 확률 분포를 예측해보자.\n포아송 분포의 평균은 \\(\\lambda\\)이며, 이 경우에는 하루 평균 교통 사고 횟수인 3으로 설정할 수 있다.\n이제 포아송 분포의 확률 질량 함수를 사용하여, 특정 날에 교통 사고가 k번 발생할 확률을 계산할 수 있다.\n\\[\nP(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\n여기서, \\(e\\)는 자연상수입니다.\n예를 들어, 특정 날에 교통 사고가 정확히 2번 발생할 확률을 계산하려면:\n\\[P(X=2) = \\frac{3^2 e^{-3}}{2!} = 0.224\\]\n한걸음 더 들어가 실세 교통사고분석시스템(TAAS) 웹사이트에서 2022년 월별 교통사고 데이터를 얻을 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\naccident_raw &lt;- read_excel(\"data/2022년_월별_교통사고.xlsx\", sheet = \"2022년도 월별 교통사고\", skip = 2)\n\naccident_tbl &lt;- accident_raw |&gt; \n  janitor::clean_names(ascii = FALSE) |&gt; \n  select(월, 사고건수 = 사고건수_건) \n\naccident_tbl\n\n# A tibble: 12 × 2\n   월    사고건수\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 01월     15894\n 2 02월     12362\n 3 03월     13620\n 4 04월     16472\n 5 05월     18065\n 6 06월     16481\n 7 07월     17115\n 8 08월     16496\n 9 09월     17216\n10 10월     18508\n11 11월     17578\n12 12월     17029\n\n\n월별 평균 사고건수를 mean() 함수를 사용해서 계산할 수 있다. 교통량은 상당히 크고, 교통사고 확률은 매우 낮기 때문에 포아송 분포로 근사를 하는 것이 가능하다. 교통사고 건수가 많기 때문에 단위를 천대 기준으로 조정하여 포아송 분포 모수 \\(\\lambda\\)를 계산한다.\n\naccident_mean &lt;- mean(accident_tbl$사고건수) / 1000\n\naccident_mean\n\n[1] 16.403\n\n\n월별로 2만대 이상 교통사고가 발생될 경우 교통사고 환자수가 급증하여 병원에 큰 부하가 걸려 사회적 문제가 된다는 가정하에 월별로 2만대 이상 교통사고가 발생할 확률을 계산해보자.\n\\(P(X \\geq 20)\\) 확률값은 전체 경우의 수에서 0 ~ 1.9 만대 사고건수가 발생할 확률을 빼주면 계산이 가능하고 다음과 같이 수식으로 표현할 수 있다.\n\\[\nP(X \\geq 20) = 1 - (P(X=0) + P(X=1) + \\ldots + P(X=19))\n\\]\n이를 R 코드로 작성하면 다음과 같이 함수형 프로그래밍 purrr 패키지 map_dbl() 함수와 포아송 함수에 \\(\\lambda = 16.403\\)를 넣어 계산이 가능하거나 내장 함수 ppois()로 직접 동일한 계산작업을 수행할 수 있다.\n\nlibrary(tidyverse)\n\naccident_prob &lt;- 1 - sum(map_dbl(0:19, ~ (accident_mean^.x * exp(-accident_mean)) / factorial(.x)))\n# 1 - ppois(19, lambda = 16.403)\n\naccident_prob\n\n[1] 0.2169535"
  },
  {
    "objectID": "cs_prussia.html#프로이센-기병-사망자",
    "href": "cs_prussia.html#프로이센-기병-사망자",
    "title": "\n1  프로이센 기병\n",
    "section": "\n1.2 프로이센 기병 사망자",
    "text": "1.2 프로이센 기병 사망자\n프로이센 기병대에서 말 발길질로 사망한 병사의 수를 1875년부터 1894년까지, 14개의 기병 군단을 대상으로 수집한 데이터(Prussian Horse-Kick Data)가 포아송 분포에 잘 적합되는 것으로 유명하다.\n\n\n\n\n원본 데이터를 디지털로 복원한 후에 고정된 기간 말 발차기 사망자수를 빈도통계를 통해 표로 정리할 수 있다. 총 관측 횟수는 \\(14 \\times 20 = 280\\) (즉, 1875년부터 1894년까지 20년간 프로이센 군단 14개를 관측), 총 사망자 병사수가 196명으로부터 평균 사망병사수를 \\(\\lambda = \\frac{196}{280} = 0.7\\) 으로 계산할 수 있다. 다음으로 포아송분포에 적합시켜서 분포로부터 말 발차기 사망자수 빈도수를 계산한다.\n\nlibrary(rvest)\nlibrary(gt)\nlibrary(gtExtras)\n\nkick_raw &lt;- read_html(x = 'https://www.randomservices.org/random/data/HorseKicks.html') |&gt; \n  html_node(\"table\") |&gt; \n  html_table()\n\n# kick_raw |&gt; \n#   write_csv(\"data/horse_kick.csv\")\n\nkick_tbl &lt;- kick_raw |&gt; \n  pivot_longer(-Year, names_to = \"군단\", values_to = \"병사수\") |&gt; \n  count(사망횟수 = 병사수, name = \"빈도수\") |&gt; \n  mutate(사망자수 = 사망횟수 * 빈도수)  |&gt; \n  mutate(포아송적합 = map_dbl(사망횟수, dpois, lambda = 196/280) * 280) |&gt; \n  mutate(포아송적합 = round(포아송적합, digits = 0)) |&gt; \n  janitor::adorn_totals(c(\"row\"), name = \"합계\")\n\nkick_tbl |&gt; \n  gt() |&gt; \n  gt_theme_538() |&gt; \n  cols_align(\"center\") |&gt; \n  gt::tab_spanner(label = \"데이터\", \n                  columns = c(사망횟수, 빈도수))\n\n\n\n\n\n\n\n        데이터\n      \n      사망자수\n      포아송적합\n    \n\n사망횟수\n      빈도수\n    \n\n\n\n0\n144\n0\n139\n\n\n1\n91\n91\n97\n\n\n2\n32\n64\n34\n\n\n3\n11\n33\n8\n\n\n4\n2\n8\n1\n\n\n합계\n280\n196\n279\n\n\n\n\n\n\n시각적으로 실제 관측한 빈도수와 포아송 분포로부터 추정한 값을 함께 겹칠 경우 일부 차이가 있긴 하지만 대체로 포아송 분포에 잘 적합됨을 확인할 수 있다.\n\nkick_tbl |&gt; \n  filter(사망횟수 != \"합계\") |&gt; \n  ggplot() +\n    geom_segment(aes(x = 사망횟수, xend = 사망횟수, y = 0, yend=빈도수),\n                 linewidth= 2) +\n    geom_point(aes(사망횟수, 포아송적합), size=3, color=\"red\") +\n    labs(x = \"말 발차기로 사망한 병사 수\",\n         y = \"빈도수\",\n         title = \"말 발길질로 인한 프로이센 병사 사망\",\n         subtitle = \"실제 관측 데이터와 포아송분포 적합 기대값\")"
  },
  {
    "objectID": "cs_prussia.html#v2-rocket",
    "href": "cs_prussia.html#v2-rocket",
    "title": "\n1  프로이센 기병\n",
    "section": "\n1.3 런던 투하 V2 로켓",
    "text": "1.3 런던 투하 V2 로켓\n2차 세계대전 중 독일은 신형 무기 V1, V2 로켓을 개발하여 전쟁 막판에 영국 런던을 폭격하여 반전을 노렸다. 독일 신형폭탄의 공격을 받은 영국에서는 독일에서 발사한 신형 폭탄이 정밀 타격한 것인지 아니면 무작위로 대충 발사를 한 것인지 데이터를 통해 검정을 하고자 한다. 1 2\n\n1.3.1 데이터\n가장 먼저 데이터를 준비한다. 데이터는 R.D. Clarke, “An Applicatin of the Poisson Distribution”을 참조한다.\n\n# 1. 기본 데이터 --------- \n\nbombs &lt;- c(\"0 개\",\"1 개\", \"2 개\",\"3 개\",\"4 개\",\"5 개 이상\")\nhit &lt;- c(229, 211, 93, 35, 7, 1)\nexpected &lt;- c(226.74, 211.39, 98.54, 30.62, 7.14, 1.57)\n\nbomb_df &lt;- tibble(bombs, hit, expected)\nbomb_df |&gt; \n  gt() |&gt; \n  gt_theme_538()\n\n\n\n\n\nbombs\n      hit\n      expected\n    \n\n\n0 개\n229\n226.74\n\n\n1 개\n211\n211.39\n\n\n2 개\n93\n98.54\n\n\n3 개\n35\n30.62\n\n\n4 개\n7\n7.14\n\n\n5 개 이상\n1\n1.57\n\n\n\n\n\n\n\n1.3.2 포아송 분포\n런던에 떨어진 폭탄이 포아송 분포, 즉 무작위로 떨어진 것이라고 가정하고 시각화를 한다. 포아송 분포는 모수가 \\(\\lambda\\) 하나만 추정하면 되기 때문에 데이터에서 모수를 추정한다.\n\\[P(\\text{ 해당 구간에서 발생한 k개 사건(k events in interval)}) = e^{-\\lambda}\\frac{\\lambda^k}{k!}\\]\n\n# 2. 포아송 분포 --------- \n\nhit &lt;- 537\narea &lt;- 576\n\n(lambda &lt;- hit/area)\n\n[1] 0.9322917\n\nggplot(bomb_df, aes(x=bombs,xend=bombs, y=0, yend=hit)) +\n  geom_segment(size=1.5) +\n  geom_point(aes(bombs, expected), size=2, color=\"red\") +\n  labs(x=\"런던 지역에 투하된 폭탄 수\", y=\"런던 지역 숫자\", title=\"영국 런던에 떨어진 V2 로켓 폭탄\",\n       subtitle=\"실제 투하 폭탄수와 포아송 분포로 추정한 폭탄수\")\n\n\n\n\n\n\n\n모수(\\(\\lambda\\))는 0.9322917로 추정된다. 이를 실제 데이터와 포아송 분포에서 나온 데이터와 겹쳐 시각화한다.\n예를 들어, 폭탄이 투하되지 않을 확률은 다음과 같다.\n\\[P(x=0) = e^{-0.9322917}\\frac{0.9322917^0}{0!} = 0.3936506\\]\n이를 R 코드로 표현하면 다음과 같다.\n\nlambda^0 *exp(-lambda) / factorial(0)\n\n[1] 0.3936506\n\n\n\n1.3.3 가설 검정\n시각적으로 살펴봤지만, 통계적 가설검정을 통해 다시 한번 런던에 투척된 폭탄이 포아송 분포를 따르는 것인지 검정해본다.\n\n귀무가설(\\(H_0\\)): 런던에 투하된 폭탄은 무작위로 떨어진 것이다. 즉, 폭탄이 떨어진 분포는 포아송 분포다.\n대립가설(\\(H_A\\)): 폭탄이 떨어진 것은 의도를 갖고 특정지역에 투하된 것이다.\n\n유의수준을 설정하고 검정통계량 \\(\\chi^2\\)을 정의해서 계산하면 귀무가설을 채택하게 된다.\n\n# 3. 통계적 검정 --------- \n\nchisq.test(bomb_df$hit, p=bomb_df$expected, rescale.p=TRUE, simulate.p.value=TRUE)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 2000 replicates)\n\ndata:  bomb_df$hit\nX-squared = 1.1709, df = NA, p-value = 0.9475\n\n# 4. 최종 데이터 ---------\n\nbomb_df$r_expected &lt;- 573 * c( dpois(0:4, lambda), 1 - sum(dpois(0:4, lambda)))\n\nbomb_df |&gt; \n  gt() |&gt; \n    gt_theme_538()\n\n\n\n\n\nbombs\n      hit\n      expected\n      r_expected\n    \n\n\n0 개\n229\n226.74\n225.561771\n\n\n1 개\n211\n211.39\n210.289359\n\n\n2 개\n93\n98.54\n98.025509\n\n\n3 개\n35\n30.62\n30.462788\n\n\n4 개\n7\n7.14\n7.100051\n\n\n5 개 이상\n1\n1.57\n1.560522\n\n\n\n\n\n\n\n1.3.4 지리정보를 통한 이해\n공간정보를 활용한 사례로 이를 공간정보에 시각화하면 다음과 같다. 물론 정확한 데이터가 없어 런던 남부에 떨어진 폭탄이 포아송 분포를 따른다고 가정하고 576개 구획으로 나눈 것에 임의로 폭탄이 떨어진 것을 시각화하면 다음과 같다.\n\n# 5. 지리정보 ---------\nlibrary(spatstat)\npar(mar = rep(0, 4))\n\n# 24*24 = 576\nsouth_london &lt;- rpoispp(lambda, win = owin(c(0, 24), c(0, 24)))\nplot(south_london, main=\"\", cex=0.5)\nabline(h = 0:24, v = 0:24, col = \"lightgray\", lty = 3)\n\n\n\n\n\n\n\n포아송 분포를 가정하고 통계적 검정도 물론 가능하다. spatstat 팩키지의 함수를 활용하여 통계적 검정을 해도 동일한 결론에 도달하게 된다.\n\nbomb_test &lt;- quadrat.test(south_london, nx = 24, ny = 24, method=\"Chisq\")\nbomb_test\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  south_london\nX2 = 563.78, df = 575, p-value = 0.7541\nalternative hypothesis: two.sided\n\nQuadrats: 24 by 24 grid of tiles\nTessellation is marked"
  },
  {
    "objectID": "data_mgmt.html",
    "href": "data_mgmt.html",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "",
    "text": "2 데이터프레임\nR 은 6가지 기본 벡터로 자료를 저장하지만, 이외에 행렬(matrix), 데이터프레임(data.frame), 리스트(list) 자료구조가 있다. 하지만, 자료분석을 위해서 데이터를 데이터셋의 형태로 구성을 해야한다. 데이터셋이 중요한 이유는 자료를 분석하기 위해서 다양한 형태의 개별 자료를 통합적으로 분석하기 위해서다. 이를 위해서 리스트 자료구조로 일단 모으게 된다. 예를 들어 개인 신용분석을 위해서는 개인의 소득, 부채, 성별, 학력 등등의 숫자형, 문자형, 요인(Factor)형 등의 자료를 데이터셋에 담아야 한다. 특히 변수와-관측값 (Variable-Observation) 형식의 자료를 분석하기 위해서는 데이터프레임(data.frame)을 사용한다. 데이터프레임은 모든 변수에 대해서 관측값이 같은 길이를 갖도록 만들어 놓은 것이다.\n데이터프레임은 data.frame 함수를 사용해서 생성한다. R 객체 구조 파악을 위해서는 간단한 자료의 경우 데이터 형식을 확인할 수 있는 1–2줄 정도의 간단한 스크립트와 명령어를 통해서 확인이 가능하지만, 복잡한 데이터의 구조를 파악하기 위해서는 summary 함수와 str 함수를 통해서 확인해야 한다.\n# 벡터를 정의한다.\nname &lt;- c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\")\ntype &lt;- c(\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \"Gas giant\", \"Gas giant\")\ndiameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)\nrotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)\nrings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n# 벡터를 합쳐서 데이터프레임을 생성\nplanets_df &lt;-data.frame(name, type, diameter, rotation, rings)"
  },
  {
    "objectID": "data_mgmt.html#측정-변수의-구분",
    "href": "data_mgmt.html#측정-변수의-구분",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.1 측정 변수의 구분",
    "text": "1.1 측정 변수의 구분\n자료가 갖는 고유한 특성을 숫자료 표현한 측정 척도에 따라, 명목형, 순서형, 구간형, 비율형 네가지로 구분된다. 측정 척도에 따라 유의미한 통계량도 함께 정해진다. [@wiener1921new] [@lee2020godeung; @stevens1946theory]\n\n명목척도(Nominal): 단순히 개체 특성 분류를 위해 숫자나 부호를 부여한 척도\n\n남자: M, 여자: F 혹은 월: 1, 화: 2, … 일:7, 혈액형: A형, B형, AB형, O형\n\n\n순서척도(Ordinal): 명목척도에 부가적으로 “순서(서열)” 정보가 추가된 척도\n\n군대계급: 사병, 장교, 장군 등\n\n\n구간척도(Interval): 순서척도에 부가적으로 “등간격” 정보가 추가된 척도\n\n구간척도는 절대 영점이 없음. 측정된 값들 간의 상대적인 차이만을 의미.\n섭씨 온도 척도에서 0도는 온도 부재를 의미하지 않음.\n덧셈과 뺄셈은 가능하지만, 곱셈과 나눗셈은 의미가 없음. 온도가 서울 10도, 제주 20도는 제주가 서울보다 온도가 2배 높다고 할 수 없음.\n온도(섭씨 20도, 화씨 68도), 시력, IQ 지수(90, 100, 110), 물가지수(기준 연도 대비 105, 110, 115) 등\n\n\n비율척도(Ratio): 구간척도에 “비율” 비교특성이 추가된 척도로 “비율 등간격” 특성이 포함됨.\n\n절대 영점은 해당 측정의 부재를 의미. 길이가 0cm는 길이가 없음을 의미. 무게 0kg는 무게가 없음을 의미.\n절대 ’0’을 가지고 사칙연산이 가능함. 10kg은 5kg의 2배.\n연령(20세, 30세, 40세), 월소득(2백만원, 3백만원, 4백만원`), TV 시청률 등.\n\n\n\n\n\n측정 분류\n\n변동계수(Coefficient of Variation, CV)는 표준편차와 평균값의 비율로 계산되는 통계값으로 다음과 같이 정의된다. 다음에서, \\(\\sigma\\)는 표준편차이고, $$ 평균이다.\n\\[CV = \\frac{\\sigma}{\\mu} \\times 100\\%\\]\nCV는 데이터의 변동성을 상대적으로 표현할 때 주로 사용된다. 특히, 두 개 이상의 변수 변동성을 비교할 때 편리하고, 변수 단위가 다르거나 평균값이 크게 다를 경우에도 CV를 사용하면 변동성을 공정하게 비교할 수 있는 장점이 있다.\n비율척도는 0이 절대적인 의미를 갖고, 모든 사칙연산이 가능한 척도 특징을 갖고 있다. CV가 0%이면, 변동성이 전혀 없다는 것을 의미하고, CV에서 0%는 절대적인 부재를 의미하며, 이것은 비율척도의 특징 중 하나다. 따라서, CV는 비율로 표현되므로, 어떤 변수의 CV가 다른 변수 CV의 2배라면, 변동성이 2배 크다는 것을 의미한다."
  },
  {
    "objectID": "data_mgmt.html#data-type-basics",
    "href": "data_mgmt.html#data-type-basics",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.2 R 자료구조",
    "text": "1.2 R 자료구조\nR에서 기본으로 사용하는 벡터 자료형은 원자 벡터(Atomic Vector) 와 리스트(List) 로 나눠진다. 원자 벡터에는 6가지 자료형이 있고, logical, integer, double, character, complex, raw, 총 6 개가 있으며 주로, 논리형, 정수형, 부동소수점형, 범주형, 4가지를 많이 사용한다. [@wickham2023r]\n\n\n자료형(Type)\n모드(Mode)\n저장모드(Storage Mode)\n\n\n\nlogical\nlogical\nlogical\n\n\ninteger\nnumeric\ninteger\n\n\ndouble\nnumeric\ndouble\n\n\ncomplex\ncomplex\ncomplex\n\n\ncharacter\ncharacter\ncharacter\n\n\nraw\nraw\nraw"
  },
  {
    "objectID": "data_mgmt.html#측정척도와-자료형",
    "href": "data_mgmt.html#측정척도와-자료형",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.3 측정척도와 자료형",
    "text": "1.3 측정척도와 자료형\n측정 척도는 크게 명목척도(Nominal), 순서척도(Ordinal), 구간척도(Interval), 그리고 비율척도(Ratio)로 분류할 수 있다.\n\n\n명목척도(Nominal)는 순서정보가 없는 범주로, logical, character나 factor 자료형으로 표현한다.\n\n순서척도(Ordinal)는 순서 정보가 추가된 명목척도로 ordered factor로 표현한다.\n\n구간척도(Interval)에서는 덧셈과 뺄셈은 가능하지만, 곱셈과 나눗셈은 의미가 없고 절대 영점도 없다. 주로 double 또는 integer 자료형으로 표현한다.\n\n비율척도(Ratio)에서는 모든 사칙연산이 가능하며, 절대 ’0’을 포함된다. 주로 double 또는 integer 자료형으로 표현한다.\n\n\n\n\n\n\n\n\n측정 척도\n설명\nR 자료형\n\n\n\n명목척도(Nominal)\n숫자나 문자로 개체 특성 분류\n\ncharacter, factor\n\n\n\n순서척도(Ordinal)\n순서 정보가 추가된 명목척도\nordered factor\n\n\n구간척도(Interval)\n덧셈, 뺄셈 가능 but 곱셈, 나눗셈 의미 없음. 절대 영점 없음\n\ndouble, integer\n\n\n\n비율척도(Ratio)\n모든 사칙연산 가능. 절대 ‘0’ 포함\n\ndouble, integer"
  },
  {
    "objectID": "data_mgmt.html#extended-data-type",
    "href": "data_mgmt.html#extended-data-type",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.4 자료형 확장",
    "text": "1.4 자료형 확장\n범주(요인), 텍스트, 날짜와 시간도 중요한 R에서 자주 사용되는 중요한 데이터 자료형으로 별도로 다뤄진다. 이를 위해서 stringr, lubridate, forcats 팩키지를 사용해서 데이터 정제작업은 물론 기계학습 예측모형 개발에 활용한다.\n\n\nR 자료형\n자료형\n예제\n\n\n\nlogical\n부울\n부도여부(Y/N), 남여\n\n\ninteger\n정수\n코로나19 감염자수\n\n\nfactor\n범주\n정당, 색상\n\n\nnumeric\n실수\n키, 몸무게, 주가, 환율\n\n\ncharacter\n텍스트\n주소, 이름, 책제목\n\n\nDate\n날짜\n생일, 투표일\n\n\n\n\n\n자료형 확장"
  },
  {
    "objectID": "data_mgmt.html#벡터와-리스트",
    "href": "data_mgmt.html#벡터와-리스트",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.5 벡터와 리스트",
    "text": "1.5 벡터와 리스트\n리스트는 재귀 벡터(recursive vector)라고도 불리는데 리스트는 다른 리스트를 포함할 수 있기 때문이다.\n따라서, 원자벡터는 동질적(homogeneous)이고, 리스트는 상대적으로 이질적(heterogeneous)이다.\n모든 벡터는 두가지 성질(Property)을 갖는데, 자료형과 길이로 이를 확인하는데 typeof()와 length() 함수를 사용해서 확인한다.\n\nlibrary(tidyverse)\n\na &lt;- list(a = 1:3,\n            b = \"a string\",\n            c = pi,\n            d = list(-1, -5) )\n  \ncat(\"자료형:\", typeof(a), \" 길이: \", length(a) ) \n\n자료형: list  길이:  4"
  },
  {
    "objectID": "data_mgmt.html#is-na-null",
    "href": "data_mgmt.html#is-na-null",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n1.6 NULL과 NA 결측값",
    "text": "1.6 NULL과 NA 결측값\n결측되었다는 없다는 것을 표시하는 방법이 두가지 필요하다. 하나는 벡터가 없다는 NULL이고, 벡터 내부에 값이 결측되었다는 NA 다. dataframe$variable &lt;- NULL 명령문을 사용하면 데이터프레임(dataframe)에 변수(variable)를 날려보내는 효과가 있다. 예를 들어 책장이 아예 없다는 의미(NULL)와 책장에 책이 없다(NA)는 다른 개념을 지칭하고 쓰임새가 다르다.\n\n\nNULL\n\n# NULL 자료형과 길이\ntypeof(NULL)\n\n[1] \"NULL\"\n\nlength(NULL)\n\n[1] 0\n\n\n\n\nNA\n\n# NA 자료형과 길이\ntypeof(NA)\n\n[1] \"logical\"\n\nlength(NA)\n\n[1] 1\n\n\n\n\nNA의 중요한 특징은 전염된다는 것이다. 즉, NA에 연산을 가하면 연산결과는 무조건 NA가 된다. NA가 7보다 큰지, 7을 더하고 빼고, 부울 연산을 하든 NA와 연산결과는 무조건 NA가 된다.\n\nNA + 7\n\n[1] NA\n\nNA / 7\n\n[1] NA\n\nNA &gt; 7\n\n[1] NA\n\n7 == NA\n\n[1] NA\n\nNA == NA\n\n[1] NA"
  },
  {
    "objectID": "data_mgmt.html#data-type-factor",
    "href": "data_mgmt.html#data-type-factor",
    "title": "\n1  데이터 관리 방법론\n",
    "section": "\n2.1 범주형, 순서형 자료형",
    "text": "2.1 범주형, 순서형 자료형\n범주형, 순서형 자료형을 생성하는 경우 주의를 기울여야 한다. factor 함수를 사용해서 요인형 자료형을 생성하는데, 내부적으로 저장공간을 효율적으로 사용하고 속도를 빠르게 하는데 유용한다. 순서를 갖는 범주형의 경우 factor 함수 내부에 levels 인자를 넣어 정의하면 순서 정보가 유지된다.\n\n# 범주형\nanimals_vector &lt;- c(\"Elephant\", \"Giraffe\", \"Donkey\", \"Horse\")\nfactor_animals_vector &lt;- factor(animals_vector)\nfactor_animals_vector\n\n[1] Elephant Giraffe  Donkey   Horse   \nLevels: Donkey Elephant Giraffe Horse\n\n# 순위형\ntemperature_vector &lt;- c(\"High\", \"Low\", \"High\",\"Low\", \"Medium\")\nfactor_temperature_vector &lt;- factor(temperature_vector, order = TRUE, levels = c(\"Low\", \"Medium\", \"High\"))\nfactor_temperature_vector\n\n[1] High   Low    High   Low    Medium\nLevels: Low &lt; Medium &lt; High\n\n\n\n# \"M\", \"F\" 수준\nsurvey_vector &lt;- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector &lt;- factor(survey_vector)\nlevels(factor_survey_vector)\n\n[1] \"F\" \"M\"\n\n# \"Female\", \"Male\" 로 변환\nlevels(factor_survey_vector) &lt;- c(\"Female\", \"Male\")\nlevels(factor_survey_vector)\n\n[1] \"Female\" \"Male\"  \n\n\n\n# 문자형 벡터와 요인 벡터\nsurvey_vector &lt;- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector &lt;- factor(survey_vector)\n\n# 문자형 벡터 요약\nsummary(survey_vector)\n\n   Length     Class      Mode \n        5 character character \n\n# 요인 벡터 요약\nsummary(factor_survey_vector)\n\nF M \n2 3"
  },
  {
    "objectID": "database_mgmt.html#데이터베이스-기본소양",
    "href": "database_mgmt.html#데이터베이스-기본소양",
    "title": "\n2  데이터베이스\n",
    "section": "\n2.1 데이터베이스 기본소양",
    "text": "2.1 데이터베이스 기본소양\n\n2.1.1 데이터 구분\n데이터는 정형데이터(RDBMS), 비정형 데이터, 반정형 데이터로 크게 나눌 수 있다. 각 자료형에 따라 장단점이 있는 것은 명확하다. 정형데이터는 분석하기 용이한 반면 확장성과 유연성이 떨어지고, 비정형 데이터는 분석하기는 다소 어려움이 있으나 확장성과 용이성에서는 장점을 갖는다. .csv, .xml, .json 파일은 그 중간적인 특성을 갖는 반정형 데이터로 분류도힌다.\n\n\n정형, 반정형, 비정형 데이터\n\n\n2.1.2 OLTP vs. OLAP\nOLTP (OnLine Transaction Processing)는 데이터 자체 처리에 중점을 둔 개념인데 반해, OLAP (OnLine Analytical Processing)은 저장된 데이터를 분석하여 꿰뚤어 볼 수 있는 능력(Insight)를 도출하는데 중심을 두고 있다.\nOLAP의 대표적인 예로 편의점 판매시점 정보관리(Point-Of-Sales, POS) 기계를 들어보자. 편의점에서 물건을 구매한 경우 다음과 같은 거래가 발생된다.\n\n고객 카드에서 현금 10,000원 인출\n편의점 통장에 10,000 지급\n명세표 출력\n\n상기 3건의 작업 프로세스가 하나의 트랜잭션(transaction) 묶어 모두 성공적으로 처리가 되어야 편의점 물건구매가 완료되도록 개발한다.\n반면에 OLAP은 데이터를 체계적으로 저장하여 데이터에 기반한 의사결정지원을 할 수 있도록 주안점을 두고 있다.\n\n\n\n\n\n\n\n\nOLAP\nOLTP\n\n\n\n목적\n트랜젝션 처리\n데이터 분석과 보고서 작성, 대쉬보드 시각화\n\n\n설계\n앱 기능 지향\n비즈니스 주제 지향\n\n\n데이터\n운영계, 실시간 최신 데이터\n정보계, 통합/이력 데이터\n\n\n크기\n기가 바이트, 스냅샷\n테라데이터, 아카이브\n\n\nSQL 쿼리\n단순 트랜잭션, 빈번한 갱신\n복잡한 집계 쿼리\n\n\n사용자\n아주 많음\n분석가 포함 일부\n\n\n\n따라서, OLTP는 운영 데이터베이스(Operational Database)로 적합하여 쓰기 업무(Write-intensive)가 많은 경우 빠르고 안전하게 레코드를 삽입(insert)하는데 특화된 반면, OLAP은 데이터 창고(Data Warehouse) 업무에 적합한데 다양한 분석업무를 수행할 때 쿼리 작업을 속도감있게 진행할 수 있어 읽기 업무(Read-intensive)에 특화되어 있다.\n\n2.1.3 DW, 데이터 호수\n전통적인 데이터베이스(Database)는 관계형 데이터베이스를 통해서 실시간 정형데이터를 빠르고 신뢰성있게 처리하는데 운영계를 지탱하는 주된 쓰임새가 있으며, 데이터 창고(Data Warehouse)는 이력 데이터를 통합하여 꿰뚤어 볼 수 있는 능력(Insight)을 제공함은 물론 보고서와 전체적인 현황을 대쉬보드를 통해 제공하는데 큰 의미가 있다. 데이터 호수(Data Lake)는 정형, 반정형, 비정형 데이터를 모두 저장하고 관리한다는 측면에서 유연성과 확장성을 내재하고 있으며 빅데이터를 분석하여 OLAP에서 추구하는 바를 한단계 더 넓혔다는 점에서 의의를 둘 수 있다. 1\n\n\n\n\n데이터 호수(Data Lake)는 특정한 구조가 없기 때문에 접근하기 용이하고 쉽게 수정하기도 용이한 반면에 데이터 창고(Data Warehouse)는 상대적으로 유연성이 떨어진다. 뿐만 아니라 데이터 과학자는 아직 결정되지 않는 비즈니스 활용 사례를 데이터 문제로 바꿔 모형을 만들고 시각화를 하는데 데이터 호수를 적합한 데 반해 비즈니스 현업전문가는 일단 전처리가 된 데이터를 데이터 창고에 넣어 특정 목적을 달성하는데 활용된다는 점에서 비교가 된다.\n\n\n\n\n\n\n\n\n데이터 호수\n데이터 창고\n\n\n\n자료구조\n원천 데이터 (Raw Data)\n전처리 된 데이터\n\n\n데이터 활용 목적\n미결정 상태\n현재 사용 중\n\n\n사용자\n데이터 과학자\n비즈니스 현업전문가\n\n\n접근성\n접근성 높고 신속한 업데이트\n변경하기 쉽지 않고 비용도 많이 소요됨.\n\n\n\n클라우드 서비스도 데이터 창고(Data Warehouse)를 기능으로 제공하고 있는데 상품명은 다음과 같다.\n\nAWS: 아마존 Redshift\nMS Azure: Azure SQL Data Warehouse\n구글: 구글 빅쿼리(Big Query)\n\n데이터 호수도 클라우드 서비스에서 제공된다. Object Storage와 함께 하둡/스파크 빅데이터 소프트웨어와 함께 검토된다.\n\nAWS: AWS S3\nMS Azure: Blob Storage / Azure Data Lake Storage\n구글: Cloud Storage\n네이버: Object Storage\n\n2.1.4 ETL과 ELT\nETL은 추출, 변환, 적재(Extract, Transform, Load)의 약자로 동일 기종 혹은 이기종의 원천데이터로부터 데이터 웨어하우스에서 쌓는 과정을 뜻하는데 변환(Transform) 과정이 무척 많은 노력이 투여된다. 반면에 ELT는 데이터를 먼저 적재한 후에 필요에 따라 변환과정을 거쳐 후속 작업에 사용한다. 데이터 호수 ELT 프로세스가 매력적으로 보이지만 데이터 카탈로그가 잘 관리되지 않는다면 데이터 늪(Data Swamp)가 될 수 있다. 2\n\n\nETL과 ELT 비교\n\n데이터 호수를 잘 관리하지 않는다면 데이터 늪에 빠질 수 있는데 메타데이터를 잘 관리하고 거버넌스를 확립해야 되고 비정형데이터도 많이 다루기 때문에 데이터 전문가 과학자를 확보하여 효율성을 높인다.\n\n\n데이터 호수와 늪\n\n\n2.1.5 데이터 통합(integration)\n원천데이터가 서로 다른 형태로 다양하게 존재하는 상황에서 데이터를 통합한다는 것은 시스템을 맞추는 것을 넘어 개념적인 데이터 모델로 정립하여야 하고 관련하여 파생된느 다양한 문제를 조화롭게 해결하는 것으로 정의할 수 있다.\n먼저 데이터를 한곳에 모은다고 하면 어떤 데이터를 모을 것인지 정의하고 클라우드 서비스를 사용한다고 하면 AWS Redshift 혹은 S3를 상정하고 혹시나 포함될 수 있는 개인정보도 사전에 식별하여 마스킹 등을 통해 익명화시켜야 되며 데이터 혈통(Data Lineage)도 구축하여 투명성과 가시성도 확보한다.\n\n\n데이터 계통도\n\n\n2.1.6 테이블 분할과 샤딩\n지금은 빅데이터 시대라 데이터가 커지게 되면 테이블에 담을 수 없는 상황이 온다. 이런 문제를 해결하기 위해 도입된 개념이 분할(Partition) 이다. 테이블 크기가 예를 들어 100GB 혹은 1TB가 되면 인덱스도 커져서 메모리에 적재가 되지 않아 쿼리 속도와 업데이트 속도가 현격히 늦어지게 된다. 이런 경우 테이블을 더 작은 단위로 쪼개는데 이를 분할(Partition)이라고 한다.\n테이블을 분할하게 될 경우 개념과 논리 데이터 모형은 동일하지만 물리 데이터 모형이 분할에 영향을 받게 된다.\n\n개념 데이터 모형(Concept Data Model): Entity, Relationship, Attributes\n\nERD(Entity Relational Diagram), UML 다이어그램\n\n\n논리 데이터 모형(Logical Data Model): 테이블, 칼럼, 관계\n\n데이터베이스 스키마와 모형: 관계형 모형, 스타 스키마(Star Sceman)\n\n분할(Partition) 혹은 샤딩(Sharding)에 영향을 받음\n\n\n물리 데이터 모형(Physical Data Model): 물리적 저장장치\n\n백업 시스템, 파티션, CPU, 저장공간 등\n\n분할(Partition) 혹은 샤딩(Sharding)의 영향을 받음.\n\n\n\n테이블을 분할하는 방법은 수평 분할 (Horizontal Partitioning)과 수직 분할 (Vertical Partitioning) 두가지 방법이 있다.\n\n\n데이터 수평 및 수직 분할\n\n데이터베이스 샤딩(databae sharding)은 테이블이 동일한 데이터베이스에 있지 않고 다른 기계에 있다는 점에서 차이가 난다. 3\n\n\n데이터베이스 샤딩(Sharding)"
  },
  {
    "objectID": "database_mgmt.html#db-design",
    "href": "database_mgmt.html#db-design",
    "title": "\n2  데이터베이스\n",
    "section": "\n2.2 데이터베이스 설계",
    "text": "2.2 데이터베이스 설계\n데이터베이스 설계(Database Design)는 데이터를 논리적으로 저장하는 방식으로 데이터베이스 모델(Database Model)을 사용한다. 데이터베이스 모델(Database Model)은 데이터베이스 구조에 대한 최상위 사양서의 역할을 한다. 일반적으로 관계형 데이터베이스 모형(Relational Database Model)을 사용하지만 NoSQL, 객체지향 DB 모형, 네트워크 DB 모델 등이 있다. 데이터베이스의 청사진으로 스키마를 사용해서 테이블, 필드, 관계, 인덱스, 뷰로 구성하여 작성한다.\n\n데이터 (Data) ← 논리 모형\n데이터베이스 모형 ← 데이터 구조에 대한 사양서\n스키마 ← 레고 블럭(Table, Field, Relation, Index, View 등)으로 데이터베이스 설계\n\n즉, 데이터를 체계적으로 구조화하는 논리모형을 먼저 구상하고 나서 사양서를 작성하고 실제 데이터베이스 설계로 들어간다.\n\n2.2.1 데이터 모형(Data Modeling)\n데이터를 저장하는 방식에 대해 데이터 모형(Data Model)을 제작하는 단계는 다음과 같다.\n\n개념 데이터 모형(Concept Data Model): Entity, Relationship, Attributes\n\nERD(Entity Relational Diagram), UML 다이어그램\n\n\n논리 데이터 모형(Logical Data Model): 테이블, 칼럼, 관계\n\n데이터베이스 스키마와 모형: 관계형 모형, 스타 스키마(Star Sceman)\n\n\n물리 데이터 모형(Physical Data Model): 물리적 저장장치\n\n백업 시스템, 파티션, CPU, 저장공간 등\n\n\n\n2.2.2 데이터 정규화 4 5\n\n관계형 데이터베이스의 설계에서 정규화(normalization)는 중복을 최소화하도록 데이터를 구조화하는 프로세스를 지칭한다. 관계형 모델의 발견자인 에드거 F. 커드는 1970년에 제 1 정규형(1NF)로 알려진 정규화의 개념을 도입하였고, 에드거 F. 커드는 이어서 제 2 정규형(2NF)과 제 3 정규형(3NF)을 1971년에 정의하였으며, 1974년에는 레이먼드 F. 보이스와 함께 보이스-코드 정규화(BCNF)를 정의하였다. 통상 관계형 데이터베이스 테이블이 제 3 정규(3NF)형이 되었으면 정규화(Normalization) 되었다고 한다. 따라서, 데이터 정규형(Normal Forms)은 1NF 부터 3NF까지가 많이 회자된다.\n\n제 1 정규형 (1 NF)\n\n각 레코드는 유일무이(unique)해야 한다. 즉, 중복(duplication)이 없어야 함.\n각 셀은 하나의 값만 가져야 함.\n\n\n제 2 정규형 (2 NF)\n\n제 1 정규형을 만족한다.\n기본키(primary key)가 한 칼럼이면 자동으로 제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 완전 함수 종속되어야 한다.\n\n\n제 3 정규형 (3 NF)\n\n제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 이행적 함수 종속이 되지 않아야 한다.\n즉, 이행(移行)적 함수 종속 (Transitive Functional Dependency)이 없어야 한다.\n함수 종속 사례로 X, Y, Z 에 대해 X → Y 이고 Y → Z 이면 X → Z 가 성립한다. 이를 Z 가 X 에 이행적으로 함수 종속되었다고 함."
  },
  {
    "objectID": "database_mgmt.html#dimensional-modeling",
    "href": "database_mgmt.html#dimensional-modeling",
    "title": "\n2  데이터베이스\n",
    "section": "\n2.3 OLAP 차원 모델링 6\n",
    "text": "2.3 OLAP 차원 모델링 6\n\n데이터베이스 모델이은 크게 탑다운(top-down) ERD 방법론과 함께 “데이터 창고(Data Warehouse)”에서 차원 모델링(DM, Dimensional Modeling)으로 나눠진다. 좀더 쉽게 이해를 하기 위해 GURU99에 나온 사례를 살펴보자. OLAP 차원 모델링은 다음 5단계로 이뤄진다.\n\n가장 먼저 비즈니스 프로세스를 하나 선택한다.\n\n마케팅, 영업, 인사 등\n\n\n얼마나 자세히 볼 것인지 상세화 수준(grain)을 정의한다.\n\n연도별, 분기별, 월별, 주별, 일별 등\n\n\n차원을 정의한다.\n\n차원은 날짜, 지점, 공장 등을 지칭한다.\n\n\n사실(Fact)을 정의한다.\n\n일자별 지점별 제품별 판매금액과 같이 숫자값이 된다.\n\n\n스키마를 제작한다.\n\n상기 정보를 바탕으로 스타 스키마(Star Schema) 혹은 눈꽃 스키마(Snowflake Schema) 를 작성한다.\n\n\n\n상기 작업결과 사장님이 원하는 월별 지점별 직급별 판매실적을 스타 스키마로 제작하게 된다\n\n\n2.3.1 스타 스키마\n차원 모델링을 방법론으로 스타 스키마를 채택하게 되면 차원 테이블(dimensional table)과 사실 테이블(fact table)을 정의해야 되는데 차원 테이블은 되도록이면 변하지 않는 속성(attributes)을 갖게 되는 반면, 차원 테이블은 측정값에 대한 레코드가 축적되며 자주 변경되게 되고 차원 테이블과 외래키(foreign key)로 연결된다."
  },
  {
    "objectID": "database_mgmt.html#chinook-database",
    "href": "database_mgmt.html#chinook-database",
    "title": "\n2  데이터베이스\n",
    "section": "\n2.4 chinook 데이터베이스",
    "text": "2.4 chinook 데이터베이스\n치눅(chinook) 데이터베이스는 Northwind 데이터베이스에서 유래되었으며, 치눅(chinooks)은 캐나다 프레리(Canadian Prairies)과 미국 대평원(Great Plains)에서 발생되는 푄 바람(fohn winds)라고 한다. lerocha/chinook-database GitHub 저장소에 다양한 데이터베이스에 사용될 수 있도록 교육용으로 잘 준비되어 있다.\n\n2.4.1 chinook ERD\n치눅 데이터베이스는 디지털 미디어 상점의 가상 데이터를 기반으로 한 공개 데이터베이스다. 치눅 데이터베이스는 앨범, 아티스트, 장르, 트랙, 재생 목록, 고객, 직원, 공급 업체를 담고 있는 테이블로 구성되어 있다.\n\n\nArtists: 아티스트의 이름과 정보를 포함합니다.\n\nAlbums: 각 앨범과 관련된 아티스트 정보를 포함합니다.\n\nTracks: 각 트랙의 이름, 앨범 ID, 장르 ID, 미디어 형식 및 가격과 같은 정보를 포함합니다.\n\nGenres: 음악의 장르(예: 록, 재즈, 팝 등) 정보를 포함합니다.\n\nMedia Types: 미디어 형식(예: MPEG 오디오 파일, ACC 오디오 파일 등) 정보를 포함합니다.\n\nPlaylists: 재생 목록과 그에 관련된 트랙 정보를 포함합니다.\n\nCustomers: 고객의 세부 정보와 구매 이력 정보를 포함합니다.\n\nInvoices & InvoiceItems: 고객의 청구서와 청구서 항목에 대한 정보를 포함합니다.\n\nEmployees: 직원 정보와 그들의 역할, 매니저, 고용 일자 등의 정보를 포함합니다.\n\nSuppliers: 공급 업체 정보를 포함합니다.\n\nChinook 데이터베이스는 실제 비즈니스 시나리오를 모방하여 복잡한 쿼리, 프로시저 및 다른 데이터베이스 작업을 수행하기 위한 교육용 플랫폼으로 사용됩니다.\n\n각 테이블간의 관계는 chinook ERD를 통해서 YugaByte DB에서 확인할 수 있다.\n\n\n치눅 데이터베이스 ERD\n\n\n2.4.2 헬로 월드\n먼저, sqlite3 chinooks 데이터베이스를 다운로드 한다. 7\n\ndownload.file(url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\", destfile = \"data/Chinook_Sqlite.sql\")\n\n쉘에서 아래 명령어로 sqlite3 데이터베이스를 생성시킨다. 제법 시간이 걸리기 때문에 chinook.db 생성이 완료된 후에 후속 분석 작업을 수행한다.\n$ sqlite3 chinook.db &lt; Chinook_Sqlite.sql\nDBI 연결시켜 chinook.db를 R에서 작업할 수 있도록 준비시킨다.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(RSQLite)\n\nchinook &lt;- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nR 마크다운에서 제대로 되는지 테스트 한다.\n\nSELECT * FROM Artist LIMIT 10;\n\n\nDisplaying records 1 - 10\n\nArtistId\nName\n\n\n\n1\nAC/DC\n\n\n2\nAccept\n\n\n3\nAerosmith\n\n\n4\nAlanis Morissette\n\n\n5\nAlice In Chains\n\n\n6\nAntônio Carlos Jobim\n\n\n7\nApocalyptica\n\n\n8\nAudioslave\n\n\n9\nBackBeat\n\n\n10\nBilly Cobham\n\n\n\n\n\nSQLite에서는 모든 테이블, 인덱스, 트리거 및 뷰에 대한 메타데이터를 저장하는 특별한 sqlite_master 테이블이 존재한다. SQLite 데이터베이스에서 테이블 목록을 다음 쿼리를 사용하여 확인한다.\n\ndbGetQuery(chinook, '\nSELECT name, type \n         FROM sqlite_master \n        WHERE type IN (\"table\", \"view\")\n')\n\n            name  type\n1          Album table\n2         Artist table\n3       Customer table\n4       Employee table\n5          Genre table\n6        Invoice table\n7    InvoiceLine table\n8      MediaType table\n9       Playlist table\n10 PlaylistTrack table\n11         Track table\n\n\n미국에서 가장 많이 팔리는 장르는 무엇인가? 라는 질의문에 대한 SQL 쿼리를 다음과 같이 작성할 수 있다. 다음 SQL 쿼리는 SQLite in R - Answering Questions with Data에서 가져왔다.\n\ntracks_usa &lt;- '\nWITH usa_tracks_sold AS\n   (\n    SELECT il.* \n      FROM InvoiceLine  il\n     INNER JOIN Invoice i on i.InvoiceId = il.InvoiceId\n     INNER JOIN Customer c on c.CustomerId = i.CustomerId\n     WHERE c.Country = \"USA\"\n   )\nSELECT g.name AS genre_name,\n       COUNT(uts.InvoiceLineId) AS no_of_tracks_sold,\n       CAST(COUNT(uts.InvoiceLineId) AS FLOAT) / (\n                                                    SELECT COUNT(*) \n                                                      FROM usa_tracks_sold\n                                                    ) AS percentage_sold\n  FROM usa_tracks_sold uts\n INNER JOIN Track  t on t.TrackId = uts.TrackId\n INNER JOIN Genre  g on g.GenreId = t.GenreId\n GROUP BY 1\n ORDER BY 2 DESC\n LIMIT 10;\n'\n\ndbGetQuery(chinook, tracks_usa)\n\n           genre_name no_of_tracks_sold percentage_sold\n1                Rock               157      0.31781377\n2               Latin                91      0.18421053\n3               Metal                64      0.12955466\n4  Alternative & Punk                50      0.10121457\n5                Jazz                22      0.04453441\n6               Blues                15      0.03036437\n7            TV Shows                14      0.02834008\n8            R&B/Soul                12      0.02429150\n9              Comedy                 8      0.01619433\n10          Classical                 8      0.01619433\n\n\n챗GPT를 사용하여 다음과 같이 질의를 하게 되면 SQL 쿼리문을 생성해준다.\n\n프롬프트: 매출 상위 10개 국가 목록을 뽑아주세요.\n\n\ndbGetQuery(chinook, '\n  SELECT \n      c.Country,\n      SUM(il.UnitPrice * il.Quantity) AS TotalRevenue\n  FROM Invoice i\n  JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n  JOIN Customer c ON i.CustomerId = c.CustomerId\n  GROUP BY c.Country\n  ORDER BY TotalRevenue DESC\n  LIMIT 10;\n')\n\n          Country TotalRevenue\n1             USA       523.06\n2          Canada       303.96\n3          France       195.10\n4          Brazil       190.10\n5         Germany       156.48\n6  United Kingdom       112.86\n7  Czech Republic        90.24\n8        Portugal        77.24\n9           India        75.26\n10          Chile        46.62"
  },
  {
    "objectID": "database_mgmt.html#chinook-helloworld",
    "href": "database_mgmt.html#chinook-helloworld",
    "title": "\n2  데이터베이스\n",
    "section": "\n2.5 헬로 월드",
    "text": "2.5 헬로 월드\n먼저, sqlite3 chinooks 데이터베이스를 다운로드 한다. 7\n\ndownload.file(url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\", destfile = \"data/Chinook_Sqlite.sql\")\n\n쉘에서 아래 명령어로 sqlite3 데이터베이스를 생성시킨다.\n$ sqlite3 chinook.db &lt; Chinook_Sqlite.sql\nDBI 연결시켜 chinook.db를 R에서 작업할 수 있도록 준비시킨다.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(RSQLite)\n\nchinook &lt;- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nR 마크다운에서 제대로 되는지 테스트 한다.\n\nSELECT * FROM Artist LIMIT 10;\n\n\nDisplaying records 1 - 10\n\nArtistId\nName\n\n\n\n1\nAC/DC\n\n\n2\nAccept\n\n\n3\nAerosmith\n\n\n4\nAlanis Morissette\n\n\n5\nAlice In Chains\n\n\n6\nAntônio Carlos Jobim\n\n\n7\nApocalyptica\n\n\n8\nAudioslave\n\n\n9\nBackBeat\n\n\n10\nBilly Cobham"
  },
  {
    "objectID": "fonts.html#font-coding",
    "href": "fonts.html#font-coding",
    "title": "\n4  글꼴\n",
    "section": "\n4.1 R 코딩 글꼴",
    "text": "4.1 R 코딩 글꼴\n문서를 위해 작성하는데 사용되는 글꼴과 R 코딩을 위해 사용되는 글꼴은 차이가 난다. 왜냐하면 R 코딩에 사용되는 글꼴은 가독성이 좋아야하고 디버깅에 용이해야 된다. 영어는 consolas 글꼴을 많이 사용하는데 무료가 아니다. 그래서 consolas에서 영감을 받은 SIL 오픈 폰트 라이선스를 따르는 Inconsolata가 R 코딩에 많이 사용되고 있다. 하지만, R코드를 작성할 때 주석을 한글로 달거나 R마크다운 작업을 할 경우 유사한 기능을 하는 한글 글꼴이 필요하다.\n\n네이버 나눔고딕 코딩글꼴\nD2 Coding 글꼴\n\n“네이버 나눔고딕 코딩글꼴”과 “D2 Coding 글꼴”을 설치하고 나서 RStudio IDE에서 “Tools” → “Global Options…”를 클릭하면 “Options”창에서 Appearance에서 Editor font:에서 설치한 코딩전용 글꼴을 선택하고 Editor theme:도 지정한다.\n\n\nD2 코딩폰트 설치"
  },
  {
    "objectID": "fonts.html#r-viz-font",
    "href": "fonts.html#r-viz-font",
    "title": "\n4  글꼴\n",
    "section": "\n4.2 ggplot 시각화 글꼴",
    "text": "4.2 ggplot 시각화 글꼴\nextrafont 팩키지에서 font_import() 함수로 운영체제(윈도우/리눅스)에 설치된 글꼴을 R로 가져온다. 그리고 나서 loadfonts() 함수를 사용해서 설치된 글꼴을 사용하는 작업흐름을 따르게 된다.\n\nlibrary(extrafont)\nfont_import(pattern = \"D2\")\n\nImporting fonts may take a few minutes, depending on the number of fonts and the speed of the system.\nContinue? [y/n] y\nScanning ttf files in C:\\Windows\\Fonts ...\nExtracting .afm files from .ttf files...\nC:\\Windows\\Fonts\\D2Coding-Ver1.3.2-20180524.ttf =&gt; C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/metrics/D2Coding-Ver1.3.2-20180524\nC:\\Windows\\Fonts\\D2CodingBold-Ver1.3.2-20180524.ttf =&gt; C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/metrics/D2CodingBold-Ver1.3.2-20180524\nC:\\Windows\\Fonts\\MOD20.TTF =&gt; C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/metrics/MOD20\nFound FontName for 3 fonts.\nScanning afm files in C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/metrics\nWriting font table in C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/fontmap/fonttable.csv\nWriting Fontmap to C:/Users/tidyverse_user/Documents/R/win-library/3.5/extrafontdb/fontmap/Fontmap...\n\nfont_import(pattern = \"Nanum\")\n\n\n4.2.1 ggplot 한글 글꼴 사례\nextrafont 패키지 loadfonts() 함수를 사용해서 ggplot에서 적용시킬 수 있는 글꼴을 불러냈다. R 내장 데이터셋 iris를 사용하여 나눔글꼴 “Nanum Pen Script”을 기본 글꼴로 적용시켰다.\n\nlibrary(tidyverse)\nlibrary(extrafont)\nloadfonts() # 로컬 PC 에서 설치된 글꼴을 불러냄!!!\n\niris %&gt;% \n  ggplot(aes(x=Sepal.Length, y=Petal.Length, color=Species)) +\n    geom_point()+\n    labs(title=\"붓꽃 데이터 한글 글꼴 적용\", color=\"붓꽃 종류\") +\n    theme_minimal(base_family = \"Nanum Pen Script\") +\n    theme(legend.position = \"top\")"
  },
  {
    "objectID": "fonts.html#font-showtext",
    "href": "fonts.html#font-showtext",
    "title": "\n4  글꼴\n",
    "section": "\n4.3 showtext 패키지 1\n",
    "text": "4.3 showtext 패키지 1\n\nextrafont 패키지를 통해 한자를 포함한 한글을 처리할 수 있었으나, extrafont는 트루타입폰트(.ttf)를 PDF 그래픽 장치에 초점을 맞춰 개발이 되었다. 따라서, 데이터과학 최종산출물이 PDF 형태 책이 아닌 경우 여러가지 면에서 다양한 한글 글꼴을 표현하는데 있어 한계가 있다.\n새로 개발된 showtext 팩키지는 Ghostscript같은 외부 소프트웨어를 활용하지 않고도 다양한 (그래픽) 글꼴을 지원한다. showtext로 R 그래프를 생성할 때, 다양한 글꼴(TrueType, OpenType, Type 1, web fonts 등)을 지원한다.\n과거 PDF와 같은 책형태로 정보를 공유하고 전달하는 방식이 주류를 이뤘다면 인터넷 등장 이후 웹으로 정보 생성과 소비가 주류로 떠오르게 되면서 글꼴에도 변화가 생겼다. 가까운 미래에는 웹을 우선시하는 글꼴이 대세를 이룰 것으로 보인다.\n\n\nshowtext 글꼴\n\n사용자가 그래프에 텍스트를 넣기 위해 R 함수에서 text()를 호출할 때 showtext가 활성화 되어 있으면 showtext 팩키지 text() 함수를 호출해서 그래픽 혹은 이미지 파일에 텍스트를 표현하고 그렇지 않는 경우는 디폴트 장치함수 text() 함수를 호출하게 되어 있다.\n내부적으로 상세 작동 로직은 글꼴 위치를 파악해서 글리프(glyph) 정보를 추출하고 비트맵 형식, 벡터그래픽 형식에 따라서 비트맵일 경우 raster() 장치함수를 호출하고, 벡터그래픽인 경우 path() 장치함수를 호출해서 기능을 수행한다.\n\n4.3.1 R 설치 글꼴 확인\nextrafont 팩키지 loadfonts() 함수를 통해 .ttf 파일 정보를 확인한다. 현재 구글 글끌 페이지에서 많은 한글 글꼴을 지원하지 않고 있다. 구글에서 전세계 글꼴을 지원하다보 동아시아 3국 대상으로 지원되는 글꼴은 적은 것으로 보인다.\n\n# 0. 환경설정 --------------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(showtext) # 글꼴, install.packages(\"showtext\")\nlibrary(extrafont)\nloadfonts()\n\n\n4.3.2 ggplot 글꼴 적용\n한글 글꼴을 바로 적용하기에 앞서 showtext 패키지 포함된 영문글꼴 적용 사례를 먼저 돌려보자. ggplot 그래픽에 적용되는 showtext 활용 기본 작업흐름은 다음과 같다.\n\n글꼴을 적재한다.\n그래픽 장치를 연다\n\nshowtext를 통해 텍스트를 표시한다고 지정한다.\n그래프를 그린다.\n장치를 닫는다.\n\n\nlibrary(tidyverse)\nlibrary(showtext)\n\n# ggplot 그래픽 ----------------------------\n\ndat &lt;- data.frame(cond = factor(rep(c(\"A\",\"B\"), each=200)), \n                  rating = c(rnorm(200),rnorm(200, mean=.8)))\n\nfont_add_google(\"Schoolbell\", \"bell\") # 글꼴 적재\n\nshowtext.begin() # 그래픽 장치 열기\n\nggplot(dat, aes(x=rating)) + \n  geom_histogram(binwidth=.5)+ \n　annotate(\"text\", 1, 2.1, family = \"bell\", size = 15, color=\"red\", label = \"histogram\")\n\n\n\n\n\n\nshowtext.end() # 그래픽 장치 닫기"
  },
  {
    "objectID": "fonts.html#font-showtext-korean-example-ttf",
    "href": "fonts.html#font-showtext-korean-example-ttf",
    "title": "\n4  글꼴\n",
    "section": "\n4.4 로컬 글꼴 적용",
    "text": "4.4 로컬 글꼴 적용\n로컬 컴퓨터에 저장된 .ttf 파일을 사용자 지정해서 가져온 후 이를 ggplot에 반영하여 한글을 R 그래프에 적용하는 것도 가능하다. showtext는 extrafont 보다 나중에 개발되어 extrafont가 로컬 컴퓨터에 설치된 글꼴을 ggplot에 구현되는데 전력을 다했다면 showtext는 이를 발판으로 나중에 개발되어 구글 폰트와 같은 인터넷 글꼴과 최근 웹출판에 대한 개념도 넣어 개발된 것이 차이점이다.\n\n# ３. 한글 그래픽 --------------------------------------------------------------------------\n## 나눔펜　스크립트\nfont_add(\"NanumBarunGothic\", \"NanumBarunGothic.ttf\")\n\nshowtext.auto()\n\np &lt;- ggplot(NULL, aes(x = 1, y = 1)) + ylim(0.8, 1.2) +\n  theme(axis.title = element_blank(), axis.ticks = element_blank(),\n        axis.text = element_blank()) +\n  annotate(\"text\", 1, 1.1, family = \"NanumBarunGothic\", size = 15, color=\"red\",\n           label = \"한글 사랑\") +\n  annotate(\"text\", 1, 0.9, label = 'korean for \"Hello, world!\"',\n           family = \"NanumBarunGothic\", size = 12)\n\nprint(p)"
  },
  {
    "objectID": "document.html#papers-ropensci",
    "href": "document.html#papers-ropensci",
    "title": "\n3  문서\n",
    "section": "\n3.1 논문과 ropensci 진행경과",
    "text": "3.1 논문과 ropensci 진행경과\n[@marwick2018packaging]\n\n\n재현가능한 과학연구를 위한 저작 방법\n\n\nropensci, “Community Call: Reproducible Research with R”"
  },
  {
    "objectID": "document.html#compendium-intro",
    "href": "document.html#compendium-intro",
    "title": "\n3  문서\n",
    "section": "\n3.2 R 팩키지와 개요서",
    "text": "3.2 R 팩키지와 개요서\nR 프로젝트, R 팩키지, 개요서(compendium)가 서로 동일한 목표를 가지고 있지만, 다소 차이가 있는 것도 사실이다. 데이터 사이언스를 하면서 코드, 데이터, 결과물을 하나의 개요서(compendium) 아래 묶어 이를 통해 재현가능한 과학기술 발전을 도모하는 것이 무엇보다 필요하다. 3 4\n\n좋은 데이터 사이언스 프로젝트 구성\n\n모든 파일은 동일한 디렉토리 아래 정돈되어 있음\n원본 데이터(raw data)는 별도 폴더에 잘 저장되어 있어야 함.\n정제된 데이터는 R 스크립트를 통해서 만들어져야 함.\n함수는 분석 스크립트와 독립되어 저장되어야 함.\n함수는 문서화가 잘 되어야 하면 (단위) 테스트도 되어 ㅎ함.\n산출물은 코드와 격리되어야 하며 일회용으로 한번 사용하고 버림.\n\nMakefile은 적절한 순서로 분석을 실행해야 함.\nREADME 파일은 프로젝트 개요를 담고 있어야 함.\n\ngit을 사용해서 R 코드와 Rmd 문서 파일은 버전 제어를 해야 함.\n\n\n\nproject/\n|-+ data-raw/   # 원본 데이터\n|-+ data/       # (R 스크립트로부터 생성된) 정제된 데이터\n|-+ R/          # 함수(Functions)\n|-+ man/        # (Roxygen으로 생성된) 함수 문서(Function documentation)\n|-+ tests/      # 테스트 (functions, Rmd)\n|-+ vignettes/  # (Rmd로 작성된) 분석결과, 원고, 보고서 등\n|\n|- Makefile    # 자동화 시키는 마스터 스크립트\n|- DESCRIPTION # 메타데이터와 의존성\n|- README      # 프로젝트 개요서\n상기와 같이 개요서를 R 팩키지를 통해 구현하게 되면 어떤 점이 좋은지 살펴보자.\n\n재현가능성: Reproducibility\n일관되고, 표준적이며, 물흐르는 듯한 프로젝트 구조: Consistent, standard, streamlined organisation\n모듈화, 문서화, 테스트 주도 철학을 증진: Promotes modular, well-documented and tested code\n공유하기 쉬움: Easy to share (zip, GitHub repo)\n설치와 실행이 쉬움: Easy to install & run (Dependencies)\nR 팩키지 제작 기계: Use R package development machinery:\nR CMD CHECK\n지속 개발/지속 배포: Continuous integration (Travis-CI)\n\ngoodpractice로 자동화된 코드리뷰: Automatic code review with goodpractice\n\npkgdown으로 프로젝트 웹사이트 제작: Easily create project websites with pkgdown"
  },
  {
    "objectID": "document.html#start-small",
    "href": "document.html#start-small",
    "title": "\n3  문서\n",
    "section": "\n3.3 시작이 반이다",
    "text": "3.3 시작이 반이다\n\n시작이 반이다 - start small\n\nproject\n|- DESCRIPTION\n|- README.md  \n|- Metadata.txt\n|\n|- data/                \n|   +- 2014ParasiteSurveyJustBrood.csv\n|   +- CedarBPLifeTable2014.csv\n|   +- NorthBPLifeTable2013.txt\n|   +- NorthBPLifeTable2014.csv|\n|- analysis/\n|   +- CodeforBPpaper.R"
  },
  {
    "objectID": "document.html#pkg-development",
    "href": "document.html#pkg-development",
    "title": "\n3  문서\n",
    "section": "\n3.4 팩키지 개발",
    "text": "3.4 팩키지 개발\n\n팩키지 개발\n\n\nproject\n|- DESCRIPTION\n|- NAMESPACE\n|- README.md\n|- LakeTrophicModelling.Rproj\n|\n|- R/\n|   +- LakeTrophicModelling-package.r\n|   +- class_prob_rf.R\n|   +- condAccuracy.R\n|   +- crossval_rf.R\n|   +- density_plot.R\n|   +- ecdf_ks_ci.R\n|   +- ecor_map.R\n|   +- getCyanoAbund.R\n|   +- getLakeIDs.R\n|   +- importancePlot.R\n|\n|- man/\n|   +- class_prob_rf.Rd\n|   +- condAccuracy.Rd\n|   +- crossval_rf.Rd\n|   +- density_plot.Rd\n|   +- ecdf_ks_ci.Rd\n|   +- ecor_map.Rd\n|   +- getCyanoAbund.Rd\n|   +- getLakeIDs.Rd\n|   +- importancePlot.Rd\n|\n|- data/                \n|   +- LakeTrophicModelling.rda\n|\n|- vignettes/\n|\n|- inst/\n|   +- doc/\n|      +- manuscript.pdf\n|   +- extdata/\n|      +- ltmData.csv\n|      +- data_def.csv"
  },
  {
    "objectID": "document.html#cicd-docker",
    "href": "document.html#cicd-docker",
    "title": "\n3  문서\n",
    "section": "\n3.5 CI/CD와 도커",
    "text": "3.5 CI/CD와 도커\nDockerfile 파일을 추가하여 환경도 재현가능하게 만들 수 있고, .travis.yml을 추가하여 CI/CD 환경도 구축할 수 있다. tests/를 추가하여 테스트 주도 개발(test-driven development, TDD)를 시도할 수 있고, 이를 통해 수작업 검증을 자동화하여 생산성과 품질을 대폭 향상시킬 수도 있다.\nproject\n|- DESCRIPTION          # project metadata and dependencies \n|- README.md            # top-level description of content and guide to users\n|- NAMESPACE            # exports R functions in the package for repeated use\n|- LICENSE              # specify the conditions of use and reuse of the code, data & text\n|- .travis.yml          # continuous integration service hook for auto-testing at each commit\n|- Dockerfile           # makes a custom isolated computational environment for the project\n|\n|- data/                # raw data, not changed once created\n|  +- my_data.csv       # data files in open formats such as TXT, CSV, TSV, etc.\n|\n|- analysis/            # any programmatic code\n|  +- my_report.Rmd     # R markdown file with narrative text interwoven with code chunks \n|  +- makefile          # builds a PDF/HTML/DOCX file from the Rmd, code, and data files\n|  +- scripts/          # code files (R, shell, etc.) used for data cleaning, analysis and visualisation \n|\n|- R/                     \n|  +- my_functions.R    # custom R functions that are used more than once throughout the project\n|\n|- man/\n|  +- my_functions.Rd   # documentation for the R functions (auto-generated when using devtools)\n|\n|- tests/\n|  +- testthat.R        # unit tests of R functions to ensure they perform as expected"
  },
  {
    "objectID": "document.html#case-by-case-template",
    "href": "document.html#case-by-case-template",
    "title": "\n3  문서\n",
    "section": "\n3.6 각 사례별 템플릿",
    "text": "3.6 각 사례별 템플릿\n\nJeff Hollister’s manuscriptPackage\nCarl Boettiger’s template\nFrancisco Rodriguez-Sanchez’s template\n\nBen Marwick’s researchcompendium\n\nrrtools: Tools for Writing Reproducible Research in R"
  },
  {
    "objectID": "document_auto.html#보고서-작성-작업흐름",
    "href": "document_auto.html#보고서-작성-작업흐름",
    "title": "\n4  보고서 자동화\n",
    "section": "\n4.1 보고서 작성 작업흐름",
    "text": "4.1 보고서 작성 작업흐름\n일반적인 보고서 작성을 위한 작업흐름과 필요한 요소는 다음과 같다. 데이터 기반 보고서를 작성할 때 데이터와 데이터를 처리하는데 필요한 R/SAS/SPSS/파이썬 스크립트, 마크다운으로 작성한 문서가 포함된다. 즉, 데이터를 데이터베이스, 웹, 파일형태로 가져오면 R 스크립트를 통해 전처리를 하고 필요한 통계량을 뽑아내고 시각적으로 히스토그램, 막대그래프, 원그래프, 시계열 추세 선 그래프 등이 포함된다. 그리고, 데이터에서 나온 다양한 통계량 및 시각적 산출물에 대한 견해와 함께 최종 보고서 작성자의 의견을 덧붙여 마무리하고 보고서 작성자, 참고문헌, 목차, 각주 등을 붙여 보고서를 완성한다.\nGUI가 아닌 CLI 방식으로 데이터를 분석한 후에 이를 .R 파일로 저장하고 나서 이를 .Rmd 파일(코드와 문서)로 함께 작성한 뒤에 이를 기계에 던져 원하는 형태의 문서 .pdf, .html, .docx 파일을 뽑아내서 웹, PC 배포문서, 저작 가능한 문서 형태로 배포한다.\n\n\n일별 통계보고서 자동화"
  },
  {
    "objectID": "document_auto.html#다수-보고서-자동-생성",
    "href": "document_auto.html#다수-보고서-자동-생성",
    "title": "\n4  보고서 자동화\n",
    "section": "\n4.2 다수 보고서 자동 생성",
    "text": "4.2 다수 보고서 자동 생성\n하루일과는 아침에 일어나서 밥을 먹고, 학교나 회사에 출근하고, 점심먹고, 오후에 놀거나 추가 작업을 하고, 퇴근을 하고 저녁을 먹고, 쉬고 잠을 자는 과정이 일반적인 일상이다. 이런 과정이 매일 매일 반복된다. 마찬가지로 보고서도 이런 일상적인 과정을 담아내야 한다. 즉, 일별로 생성되는 데이터를 동일한 형태의 보고서로 작성하게 된다. 이를 위해 상기 “일반적인 보고서 작성 흐름”을 복사해서 붙여넣기 신공을 발휘하기 보다는 데이터를 바꿔넣고 해당 데이터를 매개변수(parameter)로 던져서 일별 보고서를 자동화시킨다.\n따라서, 일자 정보를 .Rmd 파일에 전달하는 과정을 거치는 것이 필요하고, 통계보고서를 자동생성시키는 과정을 별도 스크립트로 만들어서 실행시킨다.\n\n\n주간/월간 통계보고서 자동화"
  },
  {
    "objectID": "document_auto.html#보고서-생성-자동화-r",
    "href": "document_auto.html#보고서-생성-자동화-r",
    "title": "\n4  보고서 자동화\n",
    "section": "\n4.3 보고서 생성 자동화 (R)",
    "text": "4.3 보고서 생성 자동화 (R)\n보고서 생성 자동화 프로세스를 구현하는 방법은 다양하다. RStudio 통합개발도구를 바탕으로 운영체제 쉘로 내려가지 않고 R 스크립트 내에서 작업하는 업무 구현사례는 다음과 같다.\n즉, 다양한 보고서 템프릿을 생성시켜놓는다. 일별, 주별, 월별, 년도별 보고서를 템플릿형태로 작성하고 나서 내용을 채워 넣는다.\n보고서 내용보다 변하는 것이 데이터가 된다. 2017-01-01 데이터부터 일별로 데이터가 쭉 생성된다. 이를 일자별로, 주별, 월별, 연도별로 인식한 후 이에 대한 적절한 데이터 분석결과를 담아내는 분석 R 스크립트를 활용한다.\n데이터, 보고서 콘텐츠, 데이터분석 스크립트가 모두 준비되면 이를 리포트보고서(.Rmd)가 담고 있어야 한다. 마지막으로 make_report.r을 실행하여 보고서를 자동생성 시킨다.\n\n\n주간/월간 통계보고서 자동화 구현사례\n\n\n4.3.1 보고서 Make 파일\n일자별로 보고서를 생성시키기 위해서 보고서에 일자를 .Rmd 파일에 넘겨야 한다. 우선 일자 정보를 보고서에 전달할 수 있는 형태로 만들기 위해서 다음과정을 거친다.\n\n리포트 생성일자를 자동 생성\n월, 일을 뽑아내고 0을 덧붙여 일자를 두자리로 생성\n해당 리포트는 daily_cars_report.Rmd 파일에 params 리스트로 월(dmonth)과 일(dday)을 넘김.\n출력결과는 html_document 형식으로 지정\n한국어가 깨지는 문제가 있어 인코딩은 UTF-8으로 필히 지정\n1월1일부터 5월18일까지 일별로 돌리게 되면 오류가 생겨는 경우 이를 무시하고 계속 보고서 생성시키도록 try, silent=TRUE를 지정하여 넘김.\n\n\nlibrary(stringr)\nlibrary(lubridate)\n\nreporting_date &lt;- seq(as.Date(\"2017-01-01\"), as.Date(\"2017-05-18\"), by = \"day\")\n\n\nfor(i in reporting_date) {\n    ## 날짜뽑아내기\n    xmonth &lt;- month(as.Date(i, origin=\"1970-01-01\"))\n    xday &lt;- day(as.Date(i, origin=\"1970-01-01\"))\n    dmonth &lt;- stringr::str_pad(xmonth, 2, pad=\"0\")\n    dday &lt;- stringr::str_pad(xday, 2, pad=\"0\")\n    ## 보고서 생성\n    try(\n        rmarkdown::render('./report/daily_cars_report.Rmd',\n                      params = list(\n                          dmonth = dmonth,\n                          dday   = dday),\n                      output_format=\"html_document\",\n                      output_file =paste0 (\"daily_cars_2017\", dmonth, dday, \".html\"),\n                      encoding = 'UTF-8'\n        ),\n    silent = TRUE)\n}\n\n\n4.3.2 매개변수를 받는 보고서\nmake_report.r 파일에서 정의된 매개변수(parameter)를 받는 .Rmd 파일은 다음과 같은 모양이 된다. 즉, YAML 헤더에 params에 dmonth, dday를 정의하고 이를 params$dmonth, params$dday로 문서의 변수로 적용한다. 이를 통해 일자별로 생성되는 데이터를 자동으로 분석하여 보고서를 자동생성시키게 된다.\n\n---\ntitle: 보고서 예제\ndate: '`r strftime(Sys.time(), format = \"%B %d, %Y\")`'\noutput:\n  html_document:\n    toc: yes\n    toc_depth: 2\nparams:\n    dmonth:\n        value: x\n    dday:\n        value: x\n---\n\n#```{r, include = F}\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\n\noptions(scipen = 999, stringsAsFactors = FALSE)\n#```\n\n### 3.3. 보고서 시작합니다.\n\n#```{r, sample-report}\nprint(paste(params$directory, params$file, sep = '/'))\ngetwd()\ndataset &lt;- read.csv(paste0(\"../data/cars_2017\", params$dmonth, params$dday, \".rds\"))\nglimpse(dataset)\n#```"
  },
  {
    "objectID": "document_auto.html#예측모형-보고서",
    "href": "document_auto.html#예측모형-보고서",
    "title": "\n4  보고서 자동화\n",
    "section": "\n4.4 예측모형 보고서",
    "text": "4.4 예측모형 보고서\nR 스크립트로 데이터 분석과정을 프로그래밍하게 되면, 이를 다양한 R 팩키지와 조합하여 예측모형을 담은 보고서도 손쉽게 작성할 수 있다.\n\n\n예측모형 보고서 자동화\n\n\n데이터\n\n원본작업데이터: 로그파일, 엑셀파일, RDBMS 등\nR 바이너리 파일: 아스키 형태 데이터(.csv 등)로 저장할 경우 메타 정보가 증발하여 R 작업흐름에 태워서 사용할 경우 .rds와 같은 R 바이너리 파일이 유용하다.\n보고서 HTML 파일: 최종 산출물 보고서\n\n\nR 스크립트\n\n원본 데이터를 정제하는 R 스크립트\n보고서 혹은 예측모형을 산출하는 R 스크립트\n\n\n\n\n4.4.1 원본데이터 정제\n윈도우 터미널을 열고 RScript 명령어를 실행한다.\n\nR 정제 파일: cleansing_code.R\n\n데이터 디렉토리: ../data\n\n채팅 로그 파일: abc.txt\n\n처리결과 저장 디렉토리: ../data_processed\n\n처리결과 파일명: abc_clean.rds\n\n\n\nC:\\xwMOOC_project\\code&gt; RScript cleansing_code.R \"../data\" \"abc.txt\" \"../data_processed\"\n\n[1] \"../data_processed\\\\abc_clean.rds\"\n\n\n4.4.2 보고서 생성\n정제된 데이터(.rds)를 통계분석하여 예측모형이 포함된 보고서를 생성한다.\n\nR make 파일: make_report.R\n\n보고서 Rmd 파일: report_automation.Rmd 은 make_report.R 파일 내 지정됨\n정제된 데이터 디렉토리: data_processed\n\n정제된 데이터 파일: abc_clean.rds\n\n\n\nC:\\xwMOOC_project\\report&gt; RScript make_report.R \"data_processed\" \"abc_clean.rds\"\n\nprocessing file: report_automation.Rmd\noutput file: report_automation.knit.md\n\nOutput created: report_abc_clean_file.html"
  },
  {
    "objectID": "model_value.html",
    "href": "model_value.html",
    "title": "6  예측모형 가치",
    "section": "",
    "text": "7 예측모형 사업활용 3 4 5\n예측모형을 사업에 활용하기 위해서 AUC, 민감도, 특이도와 같은 예측모형 성능파악 지표 대신에 다음과 같은 그래프와 측도를 많이 사용한다."
  },
  {
    "objectID": "model_value.html#predictive-model-x",
    "href": "model_value.html#predictive-model-x",
    "title": "6  예측모형 가치",
    "section": "\n6.1 데이터 준비",
    "text": "6.1 데이터 준비\n먼저 UCI 웹사이트에서 데이터를 다운로드 받아 압축을 풀고, 예측모형에 사용될 수 있도록 데이터를 가공한다. 약 11.3% 고객이 은행상품에 가입한 것이 파악된다.\n\n# 0. 환경설정 -----\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(modelplotr) # devtools::install_github(\"modelplot/modelplotr\")\n\n# 1. 데이터 다운로드 -----\ndownload.file(url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\", destfile = \"data//bank-additional.zip\", mode='wb')\n\n# 1.1. 압축풀기 -----\nunzip(\"data/bank-additional.zip\", exdir=\"./data\")\n\n# 1.2. 불러오기 -----\nbank_dat &lt;- read_delim(\"data/bank-additional/bank-additional-full.csv\", delim=\";\",\n                      col_types = cols(\n                              .default = col_character(),\n                              age = col_integer(),\n                              duration = col_integer(),\n                              campaign = col_integer(),\n                              pdays = col_integer(),\n                              previous = col_integer(),\n                              emp.var.rate = col_double(),\n                              cons.price.idx = col_double(),\n                              cons.conf.idx = col_double(),\n                              euribor3m = col_double(),\n                              nr.employed = col_double()))\n\n\nbank_df &lt;- bank_dat %&gt;% \n    select_('y','duration','campaign','pdays','previous','euribor3m')\n\n\nbank_df &lt;- bank_df %&gt;% \n    mutate(y = factor(y, levels=c('no', 'yes')))\n\nbank_df %&gt;% \n    count(y) %&gt;% \n    mutate(pcnt = scales::percent(n /sum(n)))"
  },
  {
    "objectID": "model_value.html#predictive-model-caret",
    "href": "model_value.html#predictive-model-caret",
    "title": "6  예측모형 가치",
    "section": "\n6.2 예측모형",
    "text": "6.2 예측모형\ncaret 팩키지를 통해서 예측모형을 개발한다. createDataPartition() 함수로 훈련/시험 데이터를 나누고, CV 방법을 통해서 최적의 모형을 개발하도록 doSNOW 팩키지로 멀티코어를 활용한 병렬처리를 가능하도록 해서 RPART, GLM, RF 모형에 따른 최적 모형을 개발한다.\n\n# 2. 예측모형 -----\n## 2.1. 훈련/시험 데이터 분할 ------\nlibrary(caret)\n\nbank_index &lt;- createDataPartition(bank_df$y, times =1, p=0.3, list=FALSE)\n\ntrain_df &lt;- bank_df[bank_index, ]\ntest_df  &lt;- bank_df[-bank_index, ]\n\n## 2.2. 모형 개발/검증 데이터셋 준비 ------\n\ncv_folds &lt;- createMultiFolds(train_df$y, k = 10, times = 3)\n\ncv_cntrl &lt;- trainControl(method = \"repeatedcv\", number = 10,\n                         repeats = 3, index = cv_folds)\n\n\n## 2.2. 모형 개발/검증 데이터셋 준비 ------\n\nlibrary(doSNOW)\n# 실행시간\nstart.time &lt;- Sys.time()\n\ncl &lt;- makeCluster(4, type = \"SOCK\")\nregisterDoSNOW(cl)\n\nbank_rpart &lt;- train(y ~ ., data = train_df, \n                    method = \"rpart\", \n                    trControl = cv_cntrl, \n                    tuneLength = 7)\n\nbank_glm   &lt;- train(y ~ ., data = train_df, \n                    method = \"glm\",\n                    family = \"binomial\",\n                    trControl = cv_cntrl, \n                    tuneLength = 7)\n\nbank_rf    &lt;- train(y ~ ., data = train_df, \n                   method = \"rf\",\n                   trControl = cv_cntrl, \n                   tuneLength = 7,\n                   importance = TRUE)\n\nstopCluster(cl)\n\ntotal.time &lt;- Sys.time() - start.time\ntotal.time\n\n# bank_rpart_m &lt;- bank_rpart$finalModel\n# bank_glm_m &lt;- bank_glm$finalModel"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-dataset",
    "href": "model_value.html#predictive-model-caret-business-dataset",
    "title": "6  예측모형 가치",
    "section": "\n7.1 이득과 향상도 데이터",
    "text": "7.1 이득과 향상도 데이터\n이득(gain), 향상도(lift) 계산을 위해서 먼저 예측모형에서 데이터를 준비한다. 필요한 데이터는 예측값(확률/스코어 점수)과 라벨이 된다.\n\nbank_rpart_pred &lt;- predict(bank_rpart, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\nbank_glm_pred   &lt;- predict(bank_glm, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\nbank_rf_pred    &lt;- predict(bank_rf, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\n\nbank_pred_df &lt;- data.frame(bank_glm_pred, bank_rpart_pred, bank_rf_pred, test_df$y) %&gt;% tbl_df %&gt;% \n    rename(prob_glm =value,\n           prob_rpart = `value.1`, \n           prob_rf = `value.2`, \n           y = `test_df.y`)\n\nbank_pred_df %&gt;% \n    sample_n(100) %&gt;% \n    DT::datatable()"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-dataset-compute",
    "href": "model_value.html#predictive-model-caret-business-dataset-compute",
    "title": "6  예측모형 가치",
    "section": "\n7.2 이득과 향상도 계산",
    "text": "7.2 이득과 향상도 계산\n예측모형에서 이득과 향상도 계산을 위한 데이터가 준비되었다면 다음 단계로 이득(gain), 향상도(lift)를 계산하여 데이터프레임으로 준비한다.\n\n7.2.1 caret 팩키지 - lift() 함수\ncaret 팩키지 lift() 함수를 통해 이득과 향상도를 계산할 수 있으나 총 표본 대비 비율로 나눠져서 사용시 주의가 요망된다.\n\nlift_df &lt;- caret::lift(y ~ prob_glm + prob_rf + prob_rpart, data=bank_pred_df, cuts=11, class=\"yes\")\n\nlift_df$data %&gt;% \n    filter(liftModelVar == 'prob_rf')  %&gt;% \n    DT::datatable()\n\n# ggplot(lift_df, value=10)\n\n\n7.2.2 gains 팩키지 - gains() 함수\ngains 팩키지 gains() 함수를 통해 이득과 향상도를 계산할 수 있으나, 그 다음 후속 작업을 위해서 데이터프레임 변환작업을 수행하면 되는데 기본적으로 향상도와 이득에 대한 중요정보를 십분위수에 맞춰 모두 계산되어 있어 이를 참조값으로 사용한다.\n\nlibrary(gains)\n\ngains_tbl &lt;- gains(actual    = as.integer(bank_pred_df$y)-1,\n                   predicted = bank_pred_df$prob_glm, \n                   groups=10)\n\ngains_df &lt;- tibble(\n    x = gains_tbl$depth,\n    obs = gains_tbl$obs,\n    cume.obs = gains_tbl$cume.obs,\n    mean.resp = gains_tbl$mean.resp, \n    cume.mean.resp = gains_tbl$cume.mean.resp,\n    cume.pct.of.total = gains_tbl$cume.pct.of.total,\n    lift = gains_tbl$lift, \n    cume.lift = gains_tbl$cume.lift,\n    mean.prediction = gains_tbl$mean.prediction,\n    min.prediction = gains_tbl$mean.prediction, \n    max.prediction = gains_tbl$max.prediction, \n    conf = gains_tbl$conf, \n    optimal = gains_tbl$optimal, \n    num.groups = gains_tbl$num.groups,\n    percents = gains_tbl$percents\n) %&gt;% \n    add_row(x=0, obs=0, cume.obs=0, mean.resp=0, cume.mean.resp=0, cume.pct.of.total=0, lift=0,\n            cume.lift=0, mean.prediction=0, min.prediction=0, max.prediction=0, conf=\"none\",\n            optimal = \"false\",num.groups=10, percents=\"false\") %&gt;% \n    arrange(x)\n\nDT::datatable(gains_df)\n\n# gains_df %&gt;% \n#     ggplot(aes(x=x, y=cume.pct.of.total)) +\n#       geom_line() +\n#       geom_point() +\n#       scale_y_continuous(limits=c(0,1)) +\n#       geom_abline(slope=1, intercept = 0)\n\n\n7.2.3 사용자 정의 함수\nLISTEN DATA, “UNDERSTAND GAIN AND LIFT CHARTS”를 참조해서 직접 사용자 정의 함수를 작성해도 가능하다.\n\ncustom_lift &lt;- function(label_var, prob_var, groups=10) {\n    \n    tmp_df &lt;- data.frame(cbind(label_var, prob_var))\n    \n    tmp_df &lt;- tmp_df %&gt;%\n        mutate(decile = ntile(desc(prob_var), groups))\n\n    gain_table &lt;- tmp_df %&gt;% \n        group_by(decile) %&gt;%\n        summarise_at(vars(label_var), funs(total = n(),\n                                           total_resp = sum(., na.rm = TRUE))) %&gt;%\n        mutate(cum_resp = cumsum(total_resp),\n               gain     = cum_resp / sum(total_resp) * 100,\n               cum_lift = gain / (decile*(100/groups)))\n    return(gain_table)\n}\n\ncustom_lift_df &lt;- custom_lift(as.integer(bank_pred_df$y)-1, bank_pred_df$prob_glm, 10)\n\nDT::datatable(custom_lift_df)"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-cumulative-gains",
    "href": "model_value.html#predictive-model-caret-business-cumulative-gains",
    "title": "6  예측모형 가치",
    "section": "\n7.3 누적 이득 그래프",
    "text": "7.3 누적 이득 그래프\n누적 이득 그래프(Cumulative gains plot)는 예측모형을 적용해서 십분위 X 까지 선택하게 되면, 실제 목표 라벨을 실제 몇 % 까지 예측/기대할 수 있는지에 대한 답을 제공한다.\nRF가 최종 예측모형으로 선정되었다면 이를 활용하여 “누적 이득 그래프”로 시각화해보고 예측모형의 사업성과를 추정해보자.\n\nlibrary(extrafont)\nloadfonts()\n\n(gains_glm_tbl &lt;- gains(actual    = as.integer(bank_pred_df$y)-1,\n                       predicted = bank_pred_df$prob_glm, \n                       groups=10))\n\ngains_glm_df &lt;- tibble(\n    x = gains_glm_tbl$depth,\n    obs = gains_glm_tbl$obs,\n    cume.obs = gains_glm_tbl$cume.obs,\n    mean.resp = gains_glm_tbl$mean.resp, \n    cume.mean.resp = gains_glm_tbl$cume.mean.resp,\n    cume.pct.of.total = gains_glm_tbl$cume.pct.of.total,\n    lift = gains_glm_tbl$lift, \n    cume.lift = gains_glm_tbl$cume.lift,\n    mean.prediction = gains_glm_tbl$mean.prediction,\n    min.prediction = gains_glm_tbl$mean.prediction, \n    max.prediction = gains_glm_tbl$max.prediction, \n    conf = gains_glm_tbl$conf, \n    optimal = gains_glm_tbl$optimal, \n    num.groups = gains_glm_tbl$num.groups,\n    percents = gains_glm_tbl$percents\n) %&gt;% \n    add_row(x=0, obs=0, cume.obs=0, mean.resp=0, cume.mean.resp=0, cume.pct.of.total=0, lift=0,\n            cume.lift=0, mean.prediction=0, min.prediction=0, max.prediction=0, conf=\"none\",\n            optimal = \"false\",num.groups=10, percents=\"false\") %&gt;% \n    arrange(x) %&gt;% \n    mutate(decile = x / 10 %&gt;% as.integer)\n\n\n\ngains_glm_df %&gt;% \n    ggplot(aes(x=decile, y=cume.pct.of.total)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_abline(slope=0.1, intercept = 0) +\n      scale_x_continuous(limits=c(0,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(labels = scales::percent) +\n      theme_minimal() +\n      labs(x=\"십분위\", y=\"누적 이득(Cumulative Gains)\", title=\"포르투칼 마케팅 캠페인 누적 이득 그래프\") \n\n\n\n포르투칼 마케팅 캠페인 누적 이득 그래프"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-lift",
    "href": "model_value.html#predictive-model-caret-business-lift",
    "title": "6  예측모형 가치",
    "section": "\n7.4 향상 리프트(Lift) 그래프",
    "text": "7.4 향상 리프트(Lift) 그래프\n누적 향상(리프트) 그래프(Cumulative lift plot)는 예측모형을 적용해서 십분위수 X까지 선택하게 되면, 예측모형을 전혀 사용하지 않을 때 대비하여 몇배나 더 효과가 있는지에 대한 대답을 제공한다.\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=cume.lift/100)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=1, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0.5,6), breaks = seq(0,6,1)) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"누적 향상도(Cumulative Lift)\", title=\"포르투칼 마케팅 캠페인 누적 향상도 그래프\") \n\n\n\n포르투칼 마케팅 캠페인 누적 향상도 그래프"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-response-plot",
    "href": "model_value.html#predictive-model-caret-business-response-plot",
    "title": "6  예측모형 가치",
    "section": "\n7.5 반응 그래프(Reponse Plot)",
    "text": "7.5 반응 그래프(Reponse Plot)\n반응 그래프(Response Plot)는 예측모형을 적합시켜서 십분위수 X를 선택하게 되면, 해당 십분위수에서 기대되는 예상 반응율을 몇 %가 되는지에 대한 답을 제공한다.\n\navg_response_pcnt &lt;- gains_glm_df$cume.mean.resp[11]\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=mean.resp)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=avg_response_pcnt, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0,0.6), breaks = seq(0,0.6,0.1), labels = scales::percent) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"평균 반응율(%)\", title=\"포르투칼 마케팅 캠페인 반응율 그래프\") \n\n\n\n포르투칼 마케팅 캠페인 반응율 그래프"
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-cumulative-response-plot",
    "href": "model_value.html#predictive-model-caret-business-cumulative-response-plot",
    "title": "6  예측모형 가치",
    "section": "\n7.6 누적 반응 그래프(Cumulative Reponse Plot)",
    "text": "7.6 누적 반응 그래프(Cumulative Reponse Plot)\n누적 반응 그래프(Cumulative Response Plot)는 예측모형을 적합시켜서 해당 십분위수 X까지 누적하여 선택하게 되면, 해당 십분위수에서 기대되는 예상 반응율을 몇 %가 되는지에 대한 답을 제공한다.\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=cume.mean.resp)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=avg_response_pcnt, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0,0.6), breaks = seq(0,0.6,0.1), labels = scales::percent) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"누적 평균 반응율(%)\", title=\"포르투칼 마케팅 캠페인 누적 반응율 그래프\") \n\n\n\n포르투칼 마케팅 캠페인 누적 반응율 그래프"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "5  모형 카드",
    "section": "",
    "text": "모형 카드(“Model Card”)는 기계 학습 모델의 성능, 사용 사례, 제한 사항 및 이상적인 운영 조건 등의 중요한 세부 사항을 기술하는 문서를 지칭한다. Model Card의 주요 목적은 기계학습 모형 사용자에게 모형 특성과 제한 사항을 명확하게 알려주어, 모형이 적절하게 사용할 수 있도록 돕는 것이다.\n모형 카드(Model Card)는 기계 학습 모델의 다양한 측면을 설명하기 위해 다음 정보를 포함하고 있다.\n\n모델 정보(Model Information)\n\n모델 이름\n버전\n사용 사례\n모델 아키텍처\n학습 알고리즘\n라이센스 정보 등\n\n데이터셋 정보(Dataset Information)\n\n학습 데이터셋 설명\n검증 데이터셋 설명\n테스트 데이터셋 설명\n데이터 수집 방법\n데이터 처리 방법 등\n\n모델 성능(Metrics)\n\n전체 성능 지표\n하위그룹별 성능 지표\n가능한 경우, 편향(bias) 분석 결과\n\n편향 및 공정성 평가(Bias & Fairness Evaluation)\n\n고려된 편향 요소\n평가 방법\n결과 및 해석\n\n사용 상황 및 제한 사항(Use-case & Limitations)\n\n추천되는 사용 사례\n추천되지 않는 사용 사례\n알려진 제한 사항\n\n모델 관리 및 업데이트(Model Management)\n업데이트 빈도\n업데이트 메커니즘\n모델의 사용 기간\n추가 정보(Additional Information)\n\n연구 논문 또는 참조 자료\n문의처\n관련 웹사이트 링크 등"
  },
  {
    "objectID": "model.html#모형-카드",
    "href": "model.html#모형-카드",
    "title": "5  모형 관리",
    "section": "5.1 모형 카드",
    "text": "5.1 모형 카드\n모형 카드(“Model Card”)는 기계 학습 모델의 성능, 사용 사례, 제한 사항 및 이상적인 운영 조건 등의 중요한 세부 사항을 기술하는 문서를 지칭한다. Model Card의 주요 목적은 기계학습 모형 사용자에게 모형 특성과 제한 사항을 명확하게 알려주어, 모형이 적절하게 사용할 수 있도록 돕는 것이다.\n모형 카드(Model Card)는 기계 학습 모델의 다양한 측면을 설명하기 위해 다음 정보를 포함하고 있다.\n\n모델 정보(Model Information)\n\n모델 이름\n버전\n사용 사례\n모델 아키텍처\n학습 알고리즘\n라이센스 정보 등\n\n데이터셋 정보(Dataset Information)\n\n학습 데이터셋 설명\n검증 데이터셋 설명\n테스트 데이터셋 설명\n데이터 수집 방법\n데이터 처리 방법 등\n\n모델 성능(Metrics)\n\n전체 성능 지표\n하위그룹별 성능 지표\n가능한 경우, 편향(bias) 분석 결과\n\n편향 및 공정성 평가(Bias & Fairness Evaluation)\n\n고려된 편향 요소\n평가 방법\n결과 및 해석\n\n사용 상황 및 제한 사항(Use-case & Limitations)\n\n추천되는 사용 사례\n추천되지 않는 사용 사례\n알려진 제한 사항\n\n모델 관리 및 업데이트(Model Management)\n\n업데이트 빈도\n업데이트 메커니즘\n모델의 사용 기간\n\n추가 정보(Additional Information)\n\n연구 논문 또는 참조 자료\n문의처\n관련 웹사이트 링크 등"
  },
  {
    "objectID": "model.html#펭귄-성별예측",
    "href": "model.html#펭귄-성별예측",
    "title": "5  모형 관리",
    "section": "5.2 펭귄 성별예측",
    "text": "5.2 펭귄 성별예측\n파머펭귄 데이터셋에서 결측값을 제거하고 기계학습모형을 훈련, 시험 데이터로 나누고 모형성능을 평가한 후 펭귄 암수성별 예측 모형을 배포한다. Code Interpreter를 사용해서 코드를 작성한다.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\npenguins = pd.read_csv('data/penguins.csv')\n\n# Preprocessing\npenguins['bill_length_mm'].fillna(penguins['bill_length_mm'].median(), inplace=True)\npenguins['bill_depth_mm'].fillna(penguins['bill_depth_mm'].median(), inplace=True)\npenguins['flipper_length_mm'].fillna(penguins['flipper_length_mm'].median(), inplace=True)\npenguins['body_mass_g'].fillna(penguins['body_mass_g'].median(), inplace=True)\npenguins = penguins.dropna(subset=['sex'])\n\n# Splitting the data into training and testing sets\nX = penguins.drop(columns=['rowid', 'sex'])\ny = penguins['sex']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# One-hot encoding categorical variables\nencoder = OneHotEncoder(drop='first')\nencoded_train = encoder.fit_transform(X_train[['species', 'island']])\nencoded_test = encoder.transform(X_test[['species', 'island']])\nX_train_encoded = pd.concat([X_train.drop(columns=['species', 'island']), pd.DataFrame(encoded_train.toarray(), index=X_train.index)], axis=1)\nX_test_encoded = pd.concat([X_test.drop(columns=['species', 'island']), pd.DataFrame(encoded_test.toarray(), index=X_test.index)], axis=1)\n\n# 데이터 프레임의 모든 열 이름을 문자열로 변환\nX_train_encoded.columns = X_train_encoded.columns.astype(str)\nX_test_encoded.columns = X_test_encoded.columns.astype(str)\n\n\n# Training a logistic regression model\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_encoded, y_train)\n\n# Predictions\ny_pred = clf.predict(X_test_encoded)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\", classification_rep)\n\nAccuracy: 0.8955223880597015\nClassification Report:               precision    recall  f1-score   support\n\n      female       0.94      0.86      0.90        36\n        male       0.85      0.94      0.89        31\n\n    accuracy                           0.90        67\n   macro avg       0.90      0.90      0.90        67\nweighted avg       0.90      0.90      0.90        67"
  },
  {
    "objectID": "model.html#성별예측-모형카드",
    "href": "model.html#성별예측-모형카드",
    "title": "5  모형 관리",
    "section": "5.3 성별예측 모형카드",
    "text": "5.3 성별예측 모형카드\n펭귄 암수성별 예측 모형에 대한 모델 카드를 다음과 같이 작성할 수 있다. 모형 카드는 펭귄 성별 예측 모델의 기본 정보와 성능, 사용 상황 및 제한 사항 등을 포함하고 있고, 필요한 경우 추가적인 정보나 세부 사항을 포함시킬 수 있다.\n\n\n5.3.1 Model Card: 펭귄 성별 예측 모델\n\n모델 정보(Model Information)\n\n모델 이름: 펭귄 성별 예측 모델\n버전: 1.0\n사용 사례: 펭귄의 특성(부리 크기, 날개 길이, 몸무게 등)을 기반으로 성별을 예측\n모델 아키텍처: 로지스틱 회귀\n학습 알고리즘: 로지스틱 회귀 학습 알고리즘\n라이센스 정보: Open Source\n\n데이터셋 정보(Dataset Information)\n\n데이터셋 출처: 제공되지 않음\n특성: species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n대상 변수: sex (male, female)\n\n모델 성능(Metrics)\n\n정확도(Accuracy): 89.5%\n여성 펭귄 정밀도(Precision for Female): 94%\n여성 펭귄 재현율(Recall for Female): 86%\n남성 펭귄 정밀도(Precision for Male): 85%\n남성 펭귄 재현율(Recall for Male): 94%\n\n편향 및 공정성 평가(Bias & Fairness Evaluation)\n\n해당 정보가 제공되지 않았으므로, 해당 항목은 생략합니다.\n\n사용 상황 및 제한 사항(Use-case & Limitations)\n\n추천되는 사용 사례: 연구 목적, 펭귄의 성별을 빠르게 예측\n추천되지 않는 사용 사례: 실제 환경에서의 중요한 의사 결정\n알려진 제한 사항: 제공된 데이터셋에만 최적화됨\n\n모델 관리 및 업데이트(Model Management)\n\n업데이트 빈도: 없음 (첫 버전)\n업데이트 메커니즘: 모델을 다시 훈련하여 업데이트\n모델의 사용 기간: 데이터가 변하지 않는 한 계속 사용 가능\n\n추가 정보(Additional Information)\n\n연구 논문 또는 참조 자료: https://arxiv.org/abs/1810.03993\n문의처: admin@r2bit.com\n관련 웹사이트 링크: https://r2bit.com/"
  },
  {
    "objectID": "u_boats.html",
    "href": "u_boats.html",
    "title": "\n8  유보드\n",
    "section": "",
    "text": "9 목표선박 데이터\nGitHub 저장소에 침몰선박에 대한 정보도 확인해보자.\ndownload.file(url = \"https://raw.githubusercontent.com/kadenhendron/uboat-data/master/data/uboat-target-data.csv\", \n              destfile = \"data/uboat-target-data.csv\", model = \"w\")\n국적별 총 침몰선박수를 표로 작성한다.\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\n\ntarget_raw &lt;- read_csv(\"data/uboat-target-data.csv\")\n\ntarget_tbl &lt;- target_raw |&gt; \n  mutate(attack_date = lubridate::mdy(attack_date)) |&gt; \n  mutate(yearmon = floor_date(attack_date, \"month\")) \n\nships_nationality &lt;- target_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality) |&gt; \n  summarise(선박수 = n()) |&gt; \n  arrange(desc(선박수))\n\ntop10_iso &lt;- tribble(~\"nationality\", ~\"iso3c\", ~\"country_name\", ~\"country_name_kr\",\n\"British\", \"GBR\",  \"United Kingdom\", \"영국\",\n\"American\", \"USA\",  \"United States\", \"미국\",\n\"기타국가\", \"\", \"ETC\", \"기타\",\n\"Norwegian\", \"NOR\",  \"Norway\", \"노르웨이\",\n\"Dutch\", \"NLD\",  \"Denmark\", \"덴마크\", # 주의: Dutch는 네덜란드 사람을 나타내며, Denmark는 덴마크를 의미합니다.\n\"Greek\", \"GRC\",  \"Greece\", \"그리스\",\n\"Soviet\", \"SUN\", \"Russia\", \"러시아\", # 주의: Soviet는 소비에트 연방을 나타냅니다. 현대의 러시아와는 다름니다.\n\"Swedish\", \"SWE\",  \"Sweden\", \"스웨덴\",\n\"Panamanian\", \"PAN\",  \"Panama\", \"파나마\",\n\"Canadian\", \"CAN\",  \"Canada\", \"캐나다\")\n\ntarget_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality, yearmon) |&gt; \n  summarise(선박수 = n()) |&gt; \n  group_by(nationality) |&gt; \n  summarise(ships_data = list(선박수)) |&gt;  \n  left_join(ships_nationality) |&gt; \n  arrange(desc(선박수)) |&gt; \n  left_join(top10_iso) |&gt; \n  mutate(iso_2 = countrycode(country_name, origin = \"country.name\", \"iso2c\")) |&gt; \n  relocate(iso_2, .before = nationality) |&gt; \n  select(iso_2, 국가명 = country_name_kr, 선박수, ships_data) |&gt; \n  mutate(비율 = 선박수 / sum(선박수)) |&gt; \n  relocate(비율, .after = 선박수) |&gt; \n  gt() |&gt; \n    gtExtras::gt_plt_sparkline(ships_data) |&gt; \n    fmt_flag(columns = iso_2) |&gt; \n    cols_label(\n      iso_2 = \"\",\n      선박수 = \"총 침몰선박수\",\n      ships_data = \"월별 선박침몰\"\n    )  |&gt; \n    gt_theme_hangul() |&gt; \n    gt::tab_header(\n      title = \"국가별 피해 침몰선박수\", \n      subtitle = \"자료출처: uboat.net\"\n    ) |&gt; \n      grand_summary_rows(\n        columns = 선박수,\n        fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n        fmt = ~ fmt_integer(.),\n        side = \"bottom\"\n      ) |&gt; \n      grand_summary_rows(\n        columns = 비율,\n        fns =  list(label = \"합계\", fn = \"sum\"),\n        fmt = ~ fmt_percent(., decimals = 0),\n        side = \"bottom\"\n      ) |&gt; \n      fmt_percent(columns = 비율, decimals = 1) |&gt; \n      fmt_integer(columns = 선박수) |&gt; \n      cols_align(\"center\") |&gt; \n      gt::fmt_missing(missing_text = \"-\")\n\n\n\n\n\n\n국가별 피해 침몰선박수\n    \n\n자료출처: uboat.net\n    \n\n\n      \n      국가명\n      총 침몰선박수\n      비율\n      월별 선박침몰\n    \n\n\n\n\nUnited Kingdom\n\n영국\n1,655\n47.9%\n\n2.0\n\n\n\n\nUnited States\n\n미국\n549\n15.9%\n\n1.0\n\n\n\n\n-\n기타\n342\n9.9%\n\n1.0\n\n\n\n\nNorway\n\n노르웨이\n311\n9.0%\n\n2.0\n\n\n\n\nDenmark\n\n덴마크\n137\n4.0%\n\n1.0\n\n\n\n\nGreece\n\n그리스\n122\n3.5%\n\n1.0\n\n\n\n\nRussian Federation\n\n러시아\n106\n3.1%\n\n1.0\n\n\n\n\nSweden\n\n스웨덴\n89\n2.6%\n\n1.0\n\n\n\n\nPanama\n\n파나마\n81\n2.3%\n\n1.0\n\n\n\n\nCanada\n\n캐나다\n66\n1.9%\n\n1.0\n\n\n\n합계\n—\n—\n3,458\n100%\n—\n침몰된 선박과 유보트 손실을 월별 시각화를 통해서 1943년을 기점으로 유보트에 대한 연합국의 대응이 효과를 발휘하여 선박손실을 급격히 줄어든 반면 유보트 손실이 급격히 올라간 것을 확인할 수 있다.\nuboat_yearmon &lt;- uboat_tbl |&gt; \n  mutate(년월 = floor_date(fate_date, \"month\")) |&gt; \n  group_by(년월) |&gt; \n  summarise(유보트수 = n())\n\ntarget_yearmon &lt;- target_tbl |&gt; \n  mutate(년월 = floor_date(attack_date, \"month\")) |&gt; \n  group_by(년월) |&gt; \n  summarise(침몰수 = n()) \n\nuboat_yearmon |&gt; \n  left_join(target_yearmon) |&gt; \n  pivot_longer(cols = 유보트수:침몰수, names_to = \"구분\", values_to = \"선박수\") |&gt; \n  ggplot(aes(x = 년월, y = 선박수, color = 구분)) +\n    geom_line() +\n    labs(title = \"침몰 유보트와 침몰 선박수 추세\",\n         x = \"\",\n         y = \"침몰선박수\",\n         caption = \"자료출처: uboats.net\") +\n    theme_korean() +\n    scale_color_manual(values = c(\"red\", \"blue\")) +\n    theme(legend.position = \"top\")"
  },
  {
    "objectID": "u_boats.html#유보트-데이터",
    "href": "u_boats.html#유보트-데이터",
    "title": "\n8  유보드\n",
    "section": "\n8.1 유보트 데이터",
    "text": "8.1 유보트 데이터\n총 1,153대 유보트가 활동했으며 전쟁이 끝난 후에 유보트 운명을 표로 정리하면 다음과 같다.\n\nlibrary(tidyverse)\nlibrary(gt)\n\nuboat_raw &lt;- read_csv(\"data/uboat-data.csv\")\n\nfate_type_eng &lt;- c(\"Sunk\", \"Scuttled\", \"Surrendered\", \"Decommissioned\", \"Missing\", \n\"Given\", \"Damaged\", \"Captured\", \"Grounded\", \"Destroyed\")\n\nfate_type_kor &lt;- c(\"침몰\", \"자침\", \"항복\", \"해체\", \"실종\", \"기증\", \"손상\", \"포획\", \"좌초\", \"파괴\")\n\nfate_translation &lt;- tibble(fate_type = fate_type_eng,\n                           fate_type_kor = fate_type_kor)\n\nuboat_raw |&gt; \n  count(fate_type, sort = TRUE, name = \"대수\") |&gt; \n  mutate(fate_kor = c(\"침몰\", \"자침\", \"항복\", \"해체\", \"실종\", \"기증\", \"손상\", \"포획\", \"좌초\", \"파괴\")) |&gt; \n  mutate(비율 = 대수 / sum(대수)) |&gt;  \n  relocate(fate_kor, .after = fate_type) |&gt; \n  gt() |&gt; \n    gt_theme_hangul() |&gt;   \n    cols_label(\n        fate_type = md(\"**fate type**\"),\n        fate_kor = md(\"**유보트 운명**\"),\n    ) |&gt; \n    grand_summary_rows(\n      columns = 대수,\n      fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n      fmt = ~ fmt_integer(.),\n      side = \"bottom\"\n    ) |&gt; \n    grand_summary_rows(\n      columns = 비율,\n      fns =  list(label = \"합계\", fn = \"sum\"),\n      fmt = ~ fmt_percent(., decimals = 0),\n      side = \"bottom\"\n    ) |&gt; \n    fmt_percent(columns = 비율, decimals = 1) |&gt; \n    cols_align(\"center\") |&gt; \n    tab_header(\n      title = \"유보트 유형별 운명\",\n      subtitle = \"자료출처: uboat.net\"\n    )\n\n\n\n\n\n\n유보트 유형별 운명\n    \n\n자료출처: uboat.net\n    \n\n\n      fate type\n      유보트 운명\n      대수\n      비율\n    \n\n\n\n\nSunk\n침몰\n640\n55.5%\n\n\n\nScuttled\n자침\n218\n18.9%\n\n\n\nSurrendered\n항복\n154\n13.4%\n\n\n\nDecommissioned\n해체\n55\n4.8%\n\n\n\nMissing\n실종\n46\n4.0%\n\n\n\nGiven\n기증\n16\n1.4%\n\n\n\nDamaged\n손상\n12\n1.0%\n\n\n\nCaptured\n포획\n5\n0.4%\n\n\n\nGrounded\n좌초\n4\n0.3%\n\n\n\nDestroyed\n파괴\n3\n0.3%\n\n\n합계\n—\n—\n1,153\n100%\n\n\n\n\n\n\n\n8.1.1 유보트 성과\n유보트가 침몰시킨 선박수(ships_sunk) 정보가 담겨있어 이를 바탕으로 활동성과를 분석해보자.\n\nuboat_tbl &lt;- uboat_raw |&gt; \n  # filter( fate_type == \"Sunk\") |&gt; \n  mutate(fate_date = lubridate::mdy(fate)) \n\nuboat_tbl |&gt; \n  group_by(fate_type) |&gt; \n  summarise(침몰선박수 = sum(ships_sunk),\n            유보트수 = n()) |&gt; \n  mutate(비율 = 침몰선박수 / sum(침몰선박수)) |&gt; \n  arrange(desc(침몰선박수)) |&gt; \n  left_join(fate_translation) |&gt; \n  relocate(fate_type_kor, .after = fate_type) |&gt; \n  relocate(유보트수, .after = fate_type_kor) |&gt; \n  gt::gt() |&gt; \n  gt_theme_hangul() |&gt; \n  cols_align(\"center\") |&gt; \n  gt::tab_header(\n    title = \"유보트 운명유형별 침몰선박수\", \n    subtitle = \"자료출처: uboat.net\"\n  ) |&gt; \n    cols_label(\n        fate_type = md(\"**fate type**\"),\n        fate_type_kor = md(\"**유보트 운명**\"),\n    ) |&gt; \n    grand_summary_rows(\n      columns = 침몰선박수,\n      fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n      fmt = ~ fmt_integer(.),\n      side = \"bottom\"\n    ) |&gt; \n    grand_summary_rows(\n      columns = 비율,\n      fns =  list(label = \"합계\", fn = \"sum\"),\n      fmt = ~ fmt_percent(., decimals = 0),\n      side = \"bottom\"\n    ) |&gt; \n    fmt_percent(columns = 비율, decimals = 1) |&gt; \n    fmt_integer(침몰선박수) |&gt; \n    tab_spanner(label = \"침몰선박\", columns = c(침몰선박수, 비율))\n\n\n\n\n\n\n유보트 운명유형별 침몰선박수\n    \n\n자료출처: uboat.net\n    \n\n\n      fate type\n      유보트 운명\n      유보트수\n      \n        침몰선박\n      \n    \n\n침몰선박수\n      비율\n    \n\n\n\n\nSunk\n침몰\n640\n1,809\n65.8%\n\n\n\nDecommissioned\n해체\n55\n373\n13.6%\n\n\n\nScuttled\n자침\n218\n265\n9.6%\n\n\n\nSurrendered\n항복\n154\n131\n4.8%\n\n\n\nMissing\n실종\n46\n101\n3.7%\n\n\n\nGiven\n기증\n16\n41\n1.5%\n\n\n\nDamaged\n손상\n12\n16\n0.6%\n\n\n\nCaptured\n포획\n5\n12\n0.4%\n\n\n\nDestroyed\n파괴\n3\n2\n0.1%\n\n\n\nGrounded\n좌초\n4\n1\n0.0%\n\n\n합계\n—\n—\n—\n2,751\n100%\n\n\n\n\n\n\n\nuboat_tbl |&gt; \n  mutate(fate_yearmon = floor_date(fate_date, \"month\")) |&gt; \n  group_by(fate_yearmon) |&gt; \n  summarise(침몰선박수 = sum(ships_sunk)) |&gt; \n  arrange(desc(침몰선박수)) |&gt; \n  ggplot(aes(x = fate_yearmon, y = 침몰선박수)) +\n    geom_line() +\n    theme_korean() +\n    labs(x =\"\",\n         title = \"월별 유보트 침몰선박수\")\n\n\n\n\n\n\n\n\nlibrary(giscoR)\nlibrary(sf)\n\nworld &lt;- gisco_get_countries()\n\nsunk_sf &lt;- uboat_tbl |&gt; \n  filter(fate_type  == \"Sunk\")  |&gt; \n  mutate(sunk_year = year(fate_date)) |&gt; \n  select(name, fat_lon, fate_lat, sunk_year) |&gt; \n  mutate(across(fat_lon:fate_lat, as.numeric)) |&gt; \n  mutate(sunk_year = as.factor(sunk_year)) |&gt; \n  drop_na() |&gt; \n  st_as_sf(coords = c(\"fat_lon\", \"fate_lat\"), crs = sf::st_crs(world))\n\nsunk_sf |&gt; \n  mutate(sunk_year = case_when(sunk_year %in% c(1939, 1940) ~ \"1939-40\",\n                   TRUE ~ sunk_year)) |&gt; \n  ggplot() +\n    geom_sf(size = 0.5, aes(color = sunk_year)) +\n    geom_sf(data = world) +\n    facet_wrap( ~ sunk_year ) +\n    theme_korean() +\n    theme(legend.position = \"none\") +\n    labs(title = \"연도별 침몰 유보트 좌표\")"
  },
  {
    "objectID": "u_boats.html#목표-데이터",
    "href": "u_boats.html#목표-데이터",
    "title": "\n8  유보드\n",
    "section": "\n8.2 목표 데이터",
    "text": "8.2 목표 데이터\n\ndownload.file(url = \"https://raw.githubusercontent.com/kadenhendron/uboat-data/master/data/uboat-target-data.csv\", \n              destfile = \"data/uboat-target-data.csv\", model = \"w\")\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\ntarget_raw &lt;- read_csv(\"data/uboat-target-data.csv\")\n\ntarget_raw |&gt; \n  count(nationality, sort = TRUE)\n\n# A tibble: 47 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 British      1655\n 2 American      549\n 3 Norwegian     311\n 4 Dutch         137\n 5 Greek         122\n 6 Soviet        106\n 7 Swedish        89\n 8 Panamanian     81\n 9 Canadian       66\n10 French         44\n# ℹ 37 more rows\n\ntarget_tbl &lt;- target_raw |&gt; \n  mutate(attack_date = lubridate::mdy(attack_date)) |&gt; \n  mutate(yearmon = floor_date(attack_date, \"month\")) \n\ntarget_tbl |&gt; \n  group_by(yearmon) |&gt; \n  summarise(tonnage = sum(tonnage)) |&gt;  \n  ggplot(aes(x = yearmon, y = tonnage)) +\n    geom_line() +\n    theme_korean() +\n    labs(title = \"월별 유보트 격침 톤수\")\n\n\n\n\n\n\n\n\ntarget_tbl |&gt; \n  mutate(국적 = case_when(str_detect(nationality, \"British\")  ~ \"영국\",\n                          str_detect(nationality, \"American\")  ~ \"미국\",\n                          TRUE ~ \"그외\")) |&gt; \n  group_by(국적, yearmon) |&gt; \n  summarise(침몰선박수 = n()) |&gt; \n  ggplot(aes(x = yearmon, y = 침몰선박수, fill = 국적)) +\n    geom_col() +\n    theme_korean() +\n    labs(title = \"월별 유보트 격침 톤수\")  \n\n\n\n\n\n\n\n\nlibrary(giscoR)\nlibrary(sf)\n\nworld &lt;- gisco_get_countries()\n\ntarget_sf &lt;- target_tbl |&gt; \n  filter(loss_type  == \"Sunk\")  |&gt; \n  select(name, attack_lat, attack_lo) |&gt; \n  drop_na() |&gt; \n  st_as_sf(coords = c(\"attack_lo\", \"attack_lat\"), crs = sf::st_crs(world))\n\nggplot() +\n  geom_sf(data = world) +\n  geom_sf(data = target_sf, size = 0.5)\n\n\n\n\n\n\n\n\nlibrary(giscoR)\nlibrary(sf)\n\nworld &lt;- gisco_get_countries()\n\nuboat_sf &lt;- uboat_raw |&gt; \n  filter(fate_type  == \"Sunk\")  |&gt; \n  select(id, fat_lon, fate_lat) |&gt; \n  mutate(across(fat_lon:fate_lat, as.numeric)) |&gt; \n  drop_na() |&gt; \n  st_as_sf(coords = c(\"fat_lon\", \"fate_lat\"), crs = sf::st_crs(world))\n\nggplot() +\n  geom_sf(data = world) +\n  geom_sf(data = uboat_sf, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n1. Hendron K. Germany’s U-Boats & Data Visualization: Can data visualization offer answers to our questions about history? 2016. https://medium.com/@kadenhendron/germany-s-u-boats-data-visualization-6e018c6c174."
  },
  {
    "objectID": "u_boats.html#목표선박-데이터",
    "href": "u_boats.html#목표선박-데이터",
    "title": "\n8  유보드\n",
    "section": "\n8.2 목표선박 데이터",
    "text": "8.2 목표선박 데이터\nGitHub 저장소에 침몰선박에 대한 정보도 확인해보자.\n\ndownload.file(url = \"https://raw.githubusercontent.com/kadenhendron/uboat-data/master/data/uboat-target-data.csv\", \n              destfile = \"data/uboat-target-data.csv\", model = \"w\")\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\n\ntarget_raw &lt;- read_csv(\"data/uboat-target-data.csv\")\n\ntarget_tbl &lt;- target_raw |&gt; \n  mutate(attack_date = lubridate::mdy(attack_date)) |&gt; \n  mutate(yearmon = floor_date(attack_date, \"month\")) \n\nships_nationality &lt;- target_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality) |&gt; \n  summarise(선박수 = n()) |&gt; \n  arrange(desc(선박수))\n\ntop10_iso &lt;- tribble(~\"nationality\", ~\"iso3c\", ~\"country_name\", ~\"country_name_kr\",\n\"British\", \"GBR\",  \"United Kingdom\", \"영국\",\n\"American\", \"USA\",  \"United States\", \"미국\",\n\"기타국가\", \"\", \"ETC\", \"기타\",\n\"Norwegian\", \"NOR\",  \"Norway\", \"노르웨이\",\n\"Dutch\", \"NLD\",  \"Denmark\", \"덴마크\", # 주의: Dutch는 네덜란드 사람을 나타내며, Denmark는 덴마크를 의미합니다.\n\"Greek\", \"GRC\",  \"Greece\", \"그리스\",\n\"Soviet\", \"SUN\", \"Russia\", \"러시아\", # 주의: Soviet는 소비에트 연방을 나타냅니다. 현대의 러시아와는 다름니다.\n\"Swedish\", \"SWE\",  \"Sweden\", \"스웨덴\",\n\"Panamanian\", \"PAN\",  \"Panama\", \"파나마\",\n\"Canadian\", \"CAN\",  \"Canada\", \"캐나다\")\n\ntarget_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality, yearmon) |&gt; \n  summarise(선박수 = n()) |&gt; \n  group_by(nationality) |&gt; \n  summarise(ships_data = list(선박수)) |&gt;  \n  left_join(ships_nationality) |&gt; \n  arrange(desc(선박수)) |&gt; \n  left_join(top10_iso) |&gt; \n  mutate(iso_2 = countrycode(country_name, origin = \"country.name\", \"iso2c\")) |&gt; \n  relocate(iso_2, .before = nationality) |&gt; \n  select(iso_2, 국가명 = country_name_kr, 선박수, ships_data) |&gt; \n  gt() |&gt; \n    gtExtras::gt_plt_sparkline(ships_data) |&gt; \n    fmt_flag(columns = iso_2) |&gt; \n    cols_label(\n      iso_2 = \"\",\n      선박수 = \"총 침몰선박수\",\n      ships_data = \"월별 선박침몰\"\n    )  |&gt; \n    gt_theme_hangul()\n\n\n\n\n\n\n      국가명\n      총 침몰선박수\n      월별 선박침몰\n    \n\n\nUnited Kingdom\n\n영국\n1655\n\n2.0\n\n\n\nUnited States\n\n미국\n549\n\n1.0\n\n\n\nNA\n기타\n342\n\n1.0\n\n\n\nNorway\n\n노르웨이\n311\n\n2.0\n\n\n\nDenmark\n\n덴마크\n137\n\n1.0\n\n\n\nGreece\n\n그리스\n122\n\n1.0\n\n\n\nRussian Federation\n\n러시아\n106\n\n1.0\n\n\n\nSweden\n\n스웨덴\n89\n\n1.0\n\n\n\nPanama\n\n파나마\n81\n\n1.0\n\n\n\nCanada\n\n캐나다\n66\n\n1.0\n\n\n\n\n\n\n\n\n\n\n\n1. Hendron K. Germany’s U-Boats & Data Visualization: Can data visualization offer answers to our questions about history? 2016. https://medium.com/@kadenhendron/germany-s-u-boats-data-visualization-6e018c6c174."
  },
  {
    "objectID": "u_boats.html#유보트-사령관",
    "href": "u_boats.html#유보트-사령관",
    "title": "\n8  유보드\n",
    "section": "\n9.1 유보트 사령관",
    "text": "9.1 유보트 사령관\nRiding tables with {gt} and {gtExtras}\n\ncommander_uboat &lt;- target_tbl |&gt; \n  count(commander, name) |&gt; \n  group_by(commander) |&gt; \n  summarise(잠수함 = str_c(name, collapse = \",\")) |&gt; \n  mutate(잠수함 = str_split(잠수함, pattern = \",\")) |&gt; \n  unnest(잠수함) |&gt; \n  group_by(commander) |&gt; \n  summarise(잠수함 = str_c(잠수함, collapse = \", \"))\n\ntarget_tbl |&gt; \n  group_by(commander) |&gt; \n  summarise(선박수 = n(),\n            사망자수 = sum(dead, na.rm = TRUE),\n            톤수 = sum(tonnage, na.rm = TRUE)) |&gt; \n  arrange(desc(선박수)) |&gt; \n  mutate(선박수_그래프 = 선박수) |&gt; \n  left_join(commander_uboat) |&gt; \n  slice_max(order_by = 선박수, n = 10) |&gt; \n  # 시각화 \n  gt::gt() |&gt; \n    cols_label(\n      commander = \"유보트 선장\",\n      선박수 = \"선박수\",\n      잠수함 = \"탑승 잠수함\",\n      선박수_그래프 = \"그래프\"\n    )  |&gt; \n    gt_theme_hangul() |&gt; \n    gt::tab_header(\n      title = \"상위 10 유보트 선장\", \n      subtitle = \"자료출처: uboat.net\"\n    ) |&gt; \n    gtExtras::gt_merge_stack(col1 = commander, col2 = 잠수함) |&gt; \n    cols_width(\n      commander ~ px(130),\n      선박수_그래프 ~ px(100)\n    ) |&gt; \n    fmt_integer(columns = is.numeric) |&gt; \n    gt_plt_bar_pct(column = 선박수_그래프, scaled = FALSE, fill = \"blue\", background = \"lightblue\") |&gt; \n    gt::tab_spanner(label = \"총 침몰선박\", columns = c(선박수, 선박수_그래프)) |&gt; \n    cols_align(\"center\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n상위 10 유보트 선장\n    \n\n자료출처: uboat.net\n    \n\n유보트 선장\n      \n        총 침몰선박\n      \n      사망자수\n      톤수\n    \n\n선박수\n      그래프\n    \n\n\n\n\nOtto Kretschmer\nU-23, U-99\n\n56\n\n846\n355,371\n\n\n\nWolfgang Lth\nU-138, U-181, U-43, U-9\n\n49\n\n1,097\n243,099\n\n\n\nJoachim Schepke\nU-100, U-19, U-3\n\n40\n\n468\n169,690\n\n\n\nErich Topp\nU-552, U-57\n\n39\n\n848\n217,286\n\n\n\nHeinrich Liebe\nU-38\n\n35\n\n418\n200,090\n\n\n\nViktor Schtze\nU-103, U-25\n\n35\n\n780\n180,073\n\n\n\nGeorg Lassen\nU-160\n\n33\n\n343\n205,843\n\n\n\nGnther Prien\nU-47\n\n33\n\n1,945\n197,376\n\n\n\nJohann Mohr\nU-124\n\n32\n\n978\n153,031\n\n\n\nErnst Bauer\nU-126\n\n29\n\n196\n142,021"
  }
]